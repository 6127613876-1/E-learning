[
    {
        "video_id": "zWg7U0OEAoE",
        "title": "Lecture - 1 Introduction to Data Structures and Algorithms",
        "url": "https://www.youtube.com/watch?v=zWg7U0OEAoE",
        "duration": 3211,
        "transcript": "Welcome \nto data structures and algorithms. We are going to learn today some basic terminology\nregarding data structures and the notations that you would be following in the rest of\nthis course. We will begin with some very simple definitions. An algorithm which is\nan outline of the steps that a program has to take or any computational procedure has\nto take. A program on the other hand is an implementation of an algorithm and it could\nbe in any programming language. Data structure is the way we need to organize the data, so\nthat it can be used effectively by the program. So you are all familiar with certain data\nstructures, an array or a list for instance. In this course you will be seeing a lot more\ndata structures in this course and you will see how to use them in various algorithms.\nWe will take a particular problem, try to solve that problem and in the process develop\ndata structures, the best way of organizing the data, associated with that problem. What\nis an algorithmic problem? An algorithmic problem is essentially, that you have a certain\nspecifications of an input as is given here and you specify what the output should be\nlike. And a specification of an input could be, here is one specification, a sorted, non\ndecreasing sequence of natural numbers of non-zero, finite length. That's an input,\nthat's a completely specified input. I have given 2 examples here of inputs which meet\nthe specification and I have not given any output specification yet here. Now what is an instance? These are 2 instances\nof the input. This is the specification for the input and you can have any possible instance,\nyou can take any sequence of sorted, non-decreasing numbers and that would form an input instance.\nSo there are many input instances possible here. An algorithm is essentially, describing the\nactions that one should take on the input instance to get the output as desired, as\nis specified. And again there can be infinitely many input instances and there can be infinitely\nmany algorithms for solving certain problem. Each one of you could do it in a slightly\ndifferent way. That brings the notion of good algorithm.\nIf there are so many different algorithms for solving a certain problem, what is a good\nalgorithm? Good algorithm for us is an efficient algorithm. Anything that is efficient is good.\nWhat is efficient? Efficient is something, which has small running time and takes less\nmemory. These will be the two measures of efficiency that we will be working with. There\ncould also be other measures of efficiency. But these are the only two things we would\nbe considering in this course. And most of our time we would be spending with the running\ntime really. Space, of course we will be analyzing the space and most of the time would be spent\nin worrying about the running time of an algorithm. And we would be interested in the efficiency\nof algorithms, as a function of the input size. So clearly you can imagine that, if I have\na small input and my algorithm running on that input or my program running on that input\nwill take less amount of time. If the input becomes 10 times larger, then the time taken\nby the program would also grow. It may it becomes 10 times, may be it becomes 20 times\nor may be it becomes 100 times, I do not know. It is this behavior of the increase in the\nrunning time, with the increase in the size of input that would also be of our interest\nto us. We will come to all of these in a short while as we go through these slides. How does one measure running time? So I said\nefficiency, running time, very important. How does one measure the running time of an\nalgorithm? One way would be, I have put down here as an experimental study. You have a\ncertain algorithm and you have to implement that algorithm, which means you have to write\na program in a certain programming language. You run the program with varying data sets,\nsome smaller, some larger data sets, some would be of some kinds and some would be of\ndifferent kinds, so varying composition. And then you clock the time that the program takes\nand clock does not mean that you should sit down near stopwatch. Perhaps you can use the\nsystem utility like let's say System. Current Time Millis (), to clock the time that the\nprogram takes and then from that you try and figure out, how good your algorithms is, so\nthat is what one would call an experimental study of the algorithm. This has certain limitations. So I put them\ndown. First you have to implement the algorithm to be able to determine how good your algorithm\nis you have to Implement it and that is already a huge overhead, considerable amount of time\nhas to be spent in doing this. When your experiments can be done only on a limited set of inputs.\nAfter all I said the number of instances is infinitely large and you can run your experiment\nonly on a small set of instances and that might not be really indicative of the time\nthat your algorithm is taking for other inputs, which you have not considered in your experiment. Further if you have two algorithms and you\nhave to decide, which one is better you have to use exactly the same platforms to do the\ncomparison. Platform I mean both the hardware and software environment. Because as you can\nimagine, different machines would make a difference, not just different machines in fact even the\nusers who are working on that system at that particular point would make a difference on\nthe running time of an algorithm. It becomes very messy, if you have to do it this way. What we are going to do in the part of this\ncourse, in fact in this very first lecture is to develop the general methodology, which\nwill help us to analyze running time of algorithms. We are going to do it as follows: we are going\nto first develop a high level description of an algorithm, a way of describing an algorithm\nand we are going to use this description to figure out the running time and not to implement\nit to any system. A methodology would help us take into account\nof all possible input instances and it would help us and also it will allow us to evaluate\nthe efficiency of the algorithm in a way that it is independent of the environment, independent\nof the platform that we are using. I said we will give the high level description\nof the algorithm. This very first point here, we will give a high level description. So,\nwhat is this? So this brings me to what? We will call this Pseudo-code. And this is how we are going to be specifying\nall our algorithms for the purposes of this course. Here is an example of pseudo code and you\nmight have seen this in your earlier courses also. What is this algorithm doing? This algorithm\ntakes an array A, which stores an integer in it and it is trying to find the maximum\nelement in this array. What I have written here is not a program because I think the\nsyntax is all wrong. But it is pseudo-code; it is a mixture of natural language and some\nhigh-level programming concepts. I am going to use a for loop, do loop, I am\ngoing to use if-then-else statement and I am going to use a while loop. But I will not\nbother about whether there should be a semicolon here or there should be colon here and I am\nnot going to bother about those are things are required by the compiler but for our understanding\nthis is completely clear what this program is doing. So what it is doing? It is keeping\ntrack of the maximum variable in a variable called current max which is initialized to\nthe first element of the array. And Then it is going to run through the remaining element\nof the array, compare them with the current maximum element. Current Max ← A [0]. If\nthe current maximum element is less than the current element, then it would update the\ncurrent max. A[i] becomes the new max and then when the loop terminates we would just\nreturn current max. If current Max < A[i] then current Max ← A[i]\nreturn current Max It is a very simple algorithm but just with\nthis pseudo-code, you are able to understand what it is doing. This will not run on any\ncomputer since it is the pseudo-code, but it conveys the idea or the concepts any question\nup to this point? That is what I saying most structured pseudo code, most structured than\nusual course but it is less for than formal programming. And How pseudo-code will look like? We will\nuse standard numeric and Boolean expressions in it. Instead of the assignment operator which is\n'=' in java, I will use ← and instead of the equality operator, an equality relationship\nin java which is '= =' the same in C, I will just use '='. I will declare methods with\nthe algorithmic name and the parameter it takes. Algorithm name (param 1, param2) I will use all kinds of programming construct\nlike if ...then statement, if ...then... [else] statement, while ... do, repeat ...until,\nfor ... do and to index array I will say A[i], A [i, j]. It should be clear in what it is\ndoing. I will use return when the procedure terminates\nand return value will tell, what the value returned by the particular procedure or a\nfunction. When I have to make a call to a method, I will specify that with the name\nof the method and the argument and what is the object that is used. Any question to this\npoint. What was the object? This is specifies the type of the value returned\nby the particular method. You will see more of this, when we come across more pseudo-code.\nHow do we analyze algorithms? First we identify what are the primitive operations in our pseudo-code.\nWhat is a primitive operation? It is a low level operation. Example is a data movement\nin ,I do an assignment from one to another, I do a control statement which is a branch\n(if... then ...else) subroutine call or return. I do arithmetic operations or logical operations\nthese are all we called as a primitive operation. • Data movement (assign)\n• Control (branch, subroutine call, return) • Arithmetic an logical operations (e.g.\naddition, comparison) In my pseudo code, I just inspect the pseudo\ncode and count the number of primitive operations that are executed by an algorithm. Let us\nsee an example of sorting. You all know what sorting is, the input is some sequence of\nnumbers and output is a permutation of the sequence which is in non decreasing order.\nWhat are  the requirements for the output? It should\nbe in non-decreasing order and it should be the permutation of the input. Any set of numbers which are in non-decreasing\norder does not make an output. Algorithm should sort the numbers that were given to it and\nnot just produce the sequence of numbers as an increasing order. Clearly the running time\nwill depends upon, number of elements (n) and often it depends upon, how sorted these\nnumbers are. If they are already in sorted order then the algorithm will not take a long\ntime. It also depends upon the particular algorithm we use. The running time would depend\nupon all these things. The first sorting technique we use is the one that you have used very\noften. Let us say when you are playing game of cards. What is the strategy you follow, when you\nare picking up a set of cards that have been dealt out to you? You like to keep them in\na sorted order in your hand. You start with the empty hand and you pick up the first card,\nthen you take the next card and insert it at the appropriate place. Suppose if I have some five cards in your\nhand already, let us say 2, 7, 9, jack and queen. Then I getting 8, so I am going to\nput it between 7 and 9. That is the right place it has to be placed in. I am inserting\nit at the appropriate place and that is why this technique is called insertion sort. I\nkeep on doing this, till I have picked up all the cards and inserted in the appropriate\nplace. So this is the pseudo-code for insertion sort.\nI will give an array of integers A contain input and output is a permutation of the original\nnumbers, such that it is sorted. The output is also going to be in the same array.\nA [1]≤ A [2]≤ _ ≤ A[n] This is the input, output specification. I\nam going to have 2 variables or indices i and j. The array is going to be sorted from\na [1] through a [j-1]. The jth Location is an element which I have to insert appropriately\nto the right place Clearly j has to vary from 2 to n. For j ←2 to n I am going to look at jth element and I put\nthat in key. Key ←A[j] I have to insert A [j] or the key in to the sorted sequence\nwhich is A [1] through A [j-1]. i.e. A [1_j-1] I am going to use the index i to do this.\nWhat is index i going to do? Index i is going to run down from j-1 down to 1. We going to\ndecrease index i, which is what we are doing in here in this while do loop. It starts with the value j-1.And what I am\ngoing to do? I have to insert 7 and I am going to move 9 to 7th location, because 9 is more\nthan 7. Then I compare 7 with 8 and 8 is still greater than 7, so I will move it right. Then\nI compare 7 with 6. As 6 is smaller than 7,Now I found the right place for 7, I would put\n7 here ,that exactly what is happening here. I run through this loop, till I find an element\nwhich is less than key. Key is the element which I am trying to insert. This loop will\ncontinue while the element, I am consider is more than key and this loop will terminate,\nwhen I see an element which is less than key or the loop will terminate when I reach i=0.\nWhile i >0 and A[i] > key do A [i+1] ← A[i] That means I have moved everything to the\nright and I should insert the element at the very first place and what am I doing Here?\nI am just shifting the element one step to the right. Do A [i+1] ← A[i] Note that I have to insert 7 at the right\nplace, so I shift 9 right to 1 step. 9th location becomes empty, then I shift 8 to 1 step, so\nthis 8th location becomes empty and now I can put 7 here. i + 1 is the index, which\nwould be the empty location eventually and i put the key there. A [i+1]← key All of\nyou can implement it. May be you would have implemented it in a slightly different way,\nthat would give you a different program, but the algorithm is essentially the same. You\nare going to find the right place for the element and insert it.Now Let us analyze this\nalgorithm. I have put down the algorithm on the left\n(There is a small mistake here there should be a left arrow Please make a correction on\nthat) .What we are going to do? A [i+1] ← A key\nLet us count. Key ← A[j]\nI ← j-1 These are all my primitive operations. Here\nI have to do primitive operations, why because I am comparing i with 0 and I am comparing\nA[i] with key, I am also taking and, so there are three primitive operations.\nwhile i >0 and A[i] > key Each of the operation takes a certain amount\nof time, depending upon the computer system you have.C1,C2,C3,C4,C5,C6 just Reflect or\njust represent the amount of time taken for these operations and they can be in any units.\nAnd here I am counting the number of times, each of these operations is executed is done\nin this entire program. Why this operation is done n times? I start\nby assigning j =2 then assign 3, 4,5,6,7 and go up to n. Then when I increment it once\nand check that there is one more, so I have counted it as n times. There might be small\nerrors in n and n + 1, that is not very important. So this roughly n times we have to do this\noperation. How about this operation? Key ← A[j] I am\ngoing to do exactly n-1 times once for 2, once for3, once for 4 up to n. That is why\nthis operation is being done up to n-1 times. Just leave the comment statement. Again the\noperation will be done exactly n-1 times. We have to look at how many times I come to\nthis statement. While I >0 and A[i] > key tj -reflects the Counts the number of times\nI have to shift an element to the right, when I am inserting the jth card in to my hand.\nIn the previous example when I am inserting 7, I had to shift 2 elements 8 and 9. is going\nto count that quantity and that is the number of times I am going to reach A[i] part of\nmy while loop. While I >0 and A[i] >key I will be checking this condition for many\ntimes. For one iteration or for the jth iteration of this for loop, I am going to reach this\ncondition for tj times. The total number of times I am saying that condition is the sum\nof tj as j goes from 2 to n. ∑nj=2 tj ,while I >0 and A[i] > key,\ndo A[i+1] ← A[i] Every time I see (A[i] >key) condition I also\ncome to A[i], I am going to be see this condition, I am going to come this statement one more\ntime then, I come here because you know the last time i see the statement I would exit\nout of here. That is why this is tj -1 where j going from 2 to n.∑nj=2 (t j-1) A [i+1] ← A key. This statement here is\nnot a part of the while loop (this is an assignment operation please correct this). So this statement\nis part of the for loop is done exactly n minus one times as the other statement. So\nthe total time this procedure takes if you knew what this constant work can be computed.\nYou do not know what tj is. tj is quantity which depends upon your instance and not problem\nthere is a difference here. Problem is one of sorting. The instance is a set of numbers,\nthe sequence of numbers that have given to you. Thus tj depends upon the instance. Let us see the difference that tj makes. If\nthe input was already sorted, then tj is always 1(tj=1). I just have to compare the element\nwith the last element and if it is larger than the last element, I would not have to\ndo anything. tj is always a 1 if the input is already in increasing order. What happens when the input is in decreasing\norder? If the input is in decreasing order, then the number that I am trying to insert\nis going to be smaller than all the numbers that that i already have because the input\nis in decreasing order the number I am trying to insert is smaller than the numbers I have\nsorted in my array .What am I going to do? I am going to compare with the 1st element,2nd\nelement,3rd element, 4th element and all the way up to the element. When I am trying to\ninsert the tj element, I am going to end up in comparing with all the other j elements\nin the array. In that case when tj is equal to j, note that the quantity becomes its summation\nof j, where j goes from 2 to n. It is of the kind and the running time n square of this\nalgorithm would be some constant time plus some other constant times n minus some other\nconstant. n(C1+C2+C3+C7)+∑nj=2 t j(C4+C5+C6)-(C2+C3+C5+C6+C7) Thus the behavior of this running time is\nmore like n square. We come to this point later, when we talk about asymptotic analysis\nbut this is what I meant by f(n square). On the other hand in the best case when tj=1,\nthe sum is just n or n-1 and in that case the total time is n times some constant plus\nn-1 times some constant minus some constant which is roughly n times some constant. So\nwhat we call linear time algorithm. On an average what would you expect? In the\nbest case it is something like that you have to compare only against one element you have\nto compare only against one element and in the worst case you have to compare about j\nelements. In the average case would expect that it would take compare against half of\nthis element In an average case if you were to take it as you comparing again j/2 , even\nwhen the summation of j/2 where j goes from 2 to n, what will this be? This will be roughly\nby (n square/4) and it behaves like n square( and will come to these points in a minute).\nThis is what I mean by the best, worst and average case. I take the size of input, suppose\nif I am interested in sorting n numbers and I look at all possible instances of these\nn numbers. It may be infinitely many, again it is not\nclear about how to do that. What is worst case?\nThe worst case is defined as the maximum possible time that your algorithm would take for any\ninstance of that size. So these are all the instances are of the same size. The best case\nwould be the smallest time that your algorithm takes and the average would be the average\nof all infinite bars. That was for the input for 1size of size n, that would give the values,\nfrom that we can compute worst case, best case and the average case. If I would consider\ninputs of all sizes then I can create a plot for each inputs size and I could figure out\nthe worst case, best case and an average case. Then I would get such a monotonically increasing\nplots. It is clear that as the size of the input increases, the time taken by your algorithm\nwill increase. It is not going to happen that your input size become larger and it takes\nlesser time. Which of this is the easiest to work with?\nWorst case is the one we will use the most. For the purpose of this course this is the\nonly measure we will be working with. Why is the worst case used often? First it provides\nan upper bound and it tells you how long your algorithm is going to take in the worst case. Many algorithms occurs fairly often. Quite\noften it is the case that the worst case that for many instances the time taken by the algorithm\nis close to the worst case .So that average case essentially becomes as bad as the worst\ncase In fact for the previous example that we saw average case was like n square squared\nand worst case was also n squared there were differences in the constant but it was roughly\nthe same.The average case might be very difficult quantity to compete because as you said average\ncase if you have to compute look at all possible instances and then take some kind of average\n.Or you have to say like, when my input instance is drawn from a certain distribution and the\nexpected time my algorithm will take is typically a much harder quantity to work and to compute\nwith. The worst case is the measure of interest\nin which we will be working with. Asymptotic analysis is the kind of thing that we have\nbeen doing so far as n and n square and the goal of this is to analyze the running time\nwhile getting rid of superficial details. We would like to say that an algorithm, which\nhas the running time of some constant times squared is the same as an algorithm which\nhas a running time of some other constant times ,because this constant is typically\nsomething which would be dependent upon the hardware that your using.\n3n2 = n2 In the previous exampleC1,C2, and C3 would\ndepend upon the computer system, the hardware, the compiler and many factors. We are not\ninterested to distinguish between such algorithms. Both of these algorithms, one which has the\nrunning time of 3 n square and another with running time n square have a quadratic behavior.\nWhen the input size doubles the running time of both of the algorithm increases four fold. That is the thing which is of interest to\nus. We are interested in capturing how the running time of algorithm increases, with\nthe size of the input in the limit. This is the crucial point here and that is what the\nsymptotic analysis is all about here. In the limit how does the running time of this algorithm\nincrease in input size. That brings us to something that some of you\nmight have seen before the \"big-oh\" O-notation. If I have functions f(n) , g (n) and n represents\nthe input size. f (n) measures the time taken by that algorithm. f (n) and g (n) are non-negative\nfunctions and also non-decreasing, because as the input size increases, the running time\ntaken by the algorithm would also increase. Both of these are non-decreasing functions\nof n and we say that f (n) is O (g (n)), if there exist constants c and , such that f\n(n)≤ c times of g (n)≥ n0 . f (n) =O(g(n)\nf (n) c g(n) for n ≥ n0 What does it mean? I have drawn two functions.\nThe function in red is f (n) and g (n) is some other function. The function in green\nis some constant times of g (n). As you can see beyond the point , c (g (n)) is always\nlarger than that of f (n). This is the way it continues even beyond. Then we would say\nthat f (n) is O (g (n) or f (n) is order (g (n)). f (n) = O (g(n)) Few examples would clarify this and we will\nsee those examples. The function f (n) =2n+6 and g (n) =n. If you look at these two functions\n2n+6 is always larger than  n and you might be wondering why this 2n+6\nis a non-linear function. That is because the scale here is an exponential scale. The\nscale increases by 2 on y-axis and similarly on x-axis. The red colored line is n and the\nblue line is 2n and the above next line is 4n. As you can see beyond the dotted line\nf (n) is less than 4 times of n. Hence the constant c is 4 and would be this point of\ncrossing beyond which 4n becomes larger than 2n+6. At what point does 4n becomes larger than\n2n+6. It is three. So becomes three. Then we say that f (n) which is 2n+6 is O (n).\n2n+6 = O (n) Let us look at another example. The function\nin red is g (n) which is n and any constant time g (n) which is as same scale as in the\nprevious slide. Any constant time g (n) will be just the same straight line displaced by\nsuitable amount. The green line will be 4 times n and it depends upon the intercept,\nbut you're n2 would be like the line which is blue in color. So there is no constant\nc such that n2 < c (n). Can you find out a constant c so that n2 < c\n(n) for n more than . We cannot find it. Any constant that you choose, I can pick a\nlarger n such that this is violated and so it is not the case that n2 is O (n). How does one figure out these things? This\nis the very simple rule. Suppose this is my function 50 n log n, I just drop all constants\nand the lower order terms. Forget the constant 50 and I get n log n. This function 50 n log\nn is O (n log n). In the function 7n-3, I drop the constant and lower order terms, I\nget 7n-3 as O (n). I have some complicated function like 8n2\nlog n+ 5n2 +n in which I just drop all lower order terms. This is the fastest growing term\nbecause this has n2 as well as log n in it. I just drop n2 , n term and also I drop my\nconstant and get n2 log n. This function is O ( log n). There is a constant c such that\nthis quantity this large sum here is less than c times n square log n for n larger than\nsum n0. In the limit this quantity (8n2 log n+5n2 +n) will be less than some constant\ntimes this quantity (O (n2 log n)). You can figure out what should be the value of c and\nn0 , for that to happen. This is a common error. The function 50 n\nlog n is also O (n5). Whether it is yes or no. It is yes, because this quantity (50 n\nlog n) in fact is ≤ 50 times n5 always, for all n and that is just a constant so this\nis O(n5). But when we use the O-notation we try and provide as strong amount as possible\ninstead of saying this statement is true we will rather call this as O (n log n)). We\nwill see more of this in subsequent slides. How are we going to use the O-notation? We\nare going to express the number of primitive operations that are executed during run of\nthe program as a function of the input size. We are going to use O-notation for that. If\nI have an algorithm which takes the number of primitive operations as O (n) and some\nother algorithm for which the number of primitive operations is O (n2 ). Then clearly the first\nalgorithm is better than the second. Why because as the input size doubles then the running\ntime of the algorithm is also going to double, while the running time of O (n2 ) algorithm\nwill increase four fold. Similarly our algorithm which has the running\ntime of O (log n) is better than the one which has running time of O (n). Thus we have a\nhierarchy of functions in the order of log n, n2,n3,n4, . There is a word of caution here. You might\nhave an algorithm whose running time is 1,000,000 n, because you may be doing some other operations.\nI cannot see how you would create such an algorithm, but you might have an algorithm\nof this running time. 1,000,000n is O (n), because this is ≤ some constant time n and\nyou might have some other algorithm with the running time of 2n2 .\nHence from what I said before, you would say that 1,000,000 n algorithm is better than\n2n2 . The one with the linear running time which is O (n) running time is better than\nO (n2). It is true but in the limit and the limit is achieved very late when n is really\nlarge. For small instances this 2 might actually take less amount of time than your 1,000,000\nn. You have to be careful about the constants also. We will do some examples of asymptotic analysis.\nI have a pseudo code and I have an array of n numbers sitting in an array called x and\nI have to output an array A, in which the element A[i] is the average of the numbers\nX [0] through X[i]. One way of doing it is, I basically have a for loop in which I compute\neach element of the array A. To compute A [10],what should I do? I just have to sum\nup X [0] through X [10], which I am doing here.\nFor j ← 0 to I do A ← a + X[j]\nA[i]← a/ (i+1) To compute A [10], i is taking the value 10\nand I am running the index j from 0-10. I am summing up the value of X from X [0] - X\n[10] in this accumulator a and then I am eventually dividing the value of this accumulator with\n11, because it is from X [0] to X [10]. That gives me the number I should have in A [10].\nI am going to repeat this for 11,12,13,14 and for all the elements. It is an algorithm and let us compute the\nrunning time. This is one step. It is executed for i number of times and initially i take\na value from 0,1,2,3 and all the way up to n-1. This entire thing is done n times. This\ngives you the total running time of roughly .\na ← a+ X[j] This one step is getting executed times and\nthis is the dominant thing. How many times the steps given below are executed?\nA[i] ← a/ (j+1) a ← 0 These steps are executed for n times.\na ← a + X[j] But the step mentioned above is getting executed roughly for some constant\nn2 times. Thus the running time of the algorithm is O (n2). It is a very simple problem but\nyou can have a better solution. What is a better solution? We will have a\nvariable S in which we would keep accumulating the X[i]. Initially S=0. When I compute A[i],\nwhich I already have in S, X [0] through X [i-1] because they used that at the last step.\nThat is the problem here. a ← a +X[j] Every time we are computing X. First we are\ncomputing X [0] + X [1], then we are computing X [0] + X [1] +X [2] and goes on. It is a\nkind of repeating computations. Why should we do that? We will have a single variable\nwhich will keep track of the sum of the prefixes. S at this point (s← s+x[i]), when I am in\nthe jth run of this loop has some of X [0] through X [i-1] and then some X[i] in it.\nTo compute jth element, I just need to divide this sum by i +1.\nS ←S +X[i] A[i] ← S/ (i+1)\nI keep this accumulator(S) around with me. When I finish the jth iteration of this loop,\nI have an S, the sum X [0] through X[i]. I can reuse it for the next step. How much time does this take? In each run\nof this loop I am just doing two primitive operations that makes an order n times, because\nthis loop is executed n times. I have been using this freely linear and quadratic, but\nthe slide given below just tells you the other terms I might be using. Linear is when an algorithm has an asymptotic\nrunning time of O (n), then we call it as a linear algorithm. If it has asymptotic running\ntime of n2 , we called it as a quadratic and logarithmic if it is log n. It is polynomial\nif it is nk for some constant k. Algorithm is called exponential if it has\nrunning time of (a n), where a is some number more than 1. Till now I have introduced only\nthe big-oh notation, we also have the big-omega notation and big-theta notation. The \"big-Omega\"\nnotation provides a lower bound. The function f (n) is omega of g (n), f (n) =Ω (g(n)) If constant time g (n) is always less than\nf(n), earlier that was more than f(n) but now it is less than f(n) in the limit, beyond\na certain as the picture given below illustrates. c g (n) f (n) for n\nf (n) is more than c (g(n)) beyond the point . That case we will say that f (n) is omega\nof g (n). f (n) =Ω (g (n)) In θ notation f (n) is θ (g (n), if there\nexist constant and such that f (n) is sandwiched between C1 g (n) and C2 g (n). Beyond a certain\npoint, f (n) lies between 1 constant time g (n) and another constant time of g (n).\nThen f (n) is θ (g (n)) where f (n) grows like g (n) in the limit. Another way of thinking\nof it is, f (n) is θ (g (n)). If f (n) is O (g (n)) and it also Ω (g (n). There are\ntwo more related asymptotic notations, one is called \"Little-oh\" notation and the other\nis called \"Little-omega\" notation. They are the non-tight analogs of Big-oh and Big-omega.\nIt is best to understand this through the analogy of real numbers. When I say that f (n) is O (g (n)) and iam\nreally saying that the function f in some sense is less than or equal to g, that in\nfact what use in definition or f (n) is less than c (g (n). The analogy with the real numbers\nis when the number is less than or equal to another number. is for and is for =. θ (g\n(n) is function and f=g are real numbers. If these are real numbers, you can talk of\nequality but you cannot talk of equality for a function unless they are equal. Little-oh\ncorresponds to strictly less than g and Little-omega corresponds to strictly more. We are not going\nto use these, infact we will use Big-oh. You should be very clear with that part. The formal definition for Little-oh is that,\nfor every constant c there should exist some such that f (n) is  n0 . f\n(n) ≤ c (g(n)) for n ≥ n0 How it is different from Big- oh? In that case I said, there exist\nc and such that this is true. Here we will say for every c there should exist an n0 . This is just one slide I had put up to show\nwhat the differences between these functions is like. I have an algorithm whose running\ntimes are like 400n, 20n log n, 2 ,2n2,n4 and 2n . Also I have listed out, the largest\nproblem size that you can solve in 1 second or 1 minute or 1 hour. The largest problem\nsize that you can solve is roughly 2500. Say you using some some constant, so it lets\nsay 2500if you had this has running time let say your this is what the problem size would\nbe like 4096. Why did you say that 4096 is larger than 2500, although 20n log n is the\nworst running time than 400n, because of the constant. You can see the differences happening.\nIf it is 2n2 then the problem size is 707 and when it is the problem size is 19. See the behavior as the time increases. An\nhour is 3600seconds and there is a huge increase in the size of the problem you solve, if it\nis linear time algorithm. Still there is a large increase, when it is n log n algorithm\nand not so large increase when it is an n2 algorithm and almost no increase when it is\n2n algorithm. If you have an algorithm whose running time is something like 2n , you cannot\nsolve for problem of more than size 100. It will take millions of years to solve it. So that is the behavior that bothers us that\nis the behavior we are interested in course that is why a asymptotic analysis is what\nwe we considering most of it. So any questions till this point so with that we are going\nto stop this lecture. Today we have looked at\nasymptotic analysis and some initial notation and terminology that we be following with\nthis course."
    },
    {
        "video_id": "FWmuxvOgh6Q",
        "title": "Best Books for Learning Data Structures and Algorithms",
        "url": "https://www.youtube.com/watch?v=FWmuxvOgh6Q",
        "duration": 841,
        "transcript": "hey guys how's it going so today i want to share a few books with you that i think will help you get better at data structures and algorithms these books are not only useful if you're trying to learn data structures and algorithms for your school or your day-to-day work as a software engineer but also with technical interviews in mind so without further ado let's begin before you begin if you're new here i make videos on software engineering productivity technical interviews and that kind of stuff and if you're into similar things please subscribe and hit the notification bell icon so that you don't miss any new videos all right let's begin so the first book i recommend is this little guy called computer science distilled i came across this book probably a year ago and i think it's an amazing book just to get your head around all the topics that exist in computer science just by looking at the size of the book you'll probably guess that this is probably not an exhaustive list or a detailed explanation of the material but i think it's an amazing starting point that will at least get you familiar with the topics that exist in the space it briefly touches all the topics that you need to learn in order to be good at data structures and algorithms especially for your technical interviews it's also very light-hearted and has a good sense of humor so it's not very dense to read so even if you are just a beginner software engineer or you just started taking classes or you're switching from a different field this is a really nice and easy read to get started so this book is pretty interesting it covers your basics like ideas logic counting probability that you'll need as foundations before even starting algorithms it also has the introduction to your big on notation counting time and how things get slower or faster as your data size increases then it has the strategy section that will cover like iteration recursion backtracking dynamic programming again i can't stress enough that this is just an introduction to all these topics i mean some of these topics are huge like dynamic programming and they cover that in three pages like there's no way but the goal of this book isn't to make you an expert on any of this topic it's basically to help you sort of build the dictionary of topics in your head let's look at an example of what i mean right like there's a section here that talks about hash table it says the hash table is a data structure that allows finding items in constant time searching for an item takes constant amount of time whether you're searching among 10 million or just 10 items okay you get the rough idea you don't know in depth about what a hash table is but it looks like it is pretty efficient and it stores things and you can retrieve them in constant time by now if you've followed this book you've probably read the bigger notation so you kind of have a rough idea about what constant time means so that's cool similarly to the array the hash requires pre-allocating a big chunk of sequential memory to store data okay but unlike the array the items are not stored in ordered sequence the position of an item occupies is magically given by a hash function so this is what i mean this book introduces the topic to you but there are some of the harder topic like how the lookup is constant time or how the hash table gives you such performance is sort of magic right it's not going to explain you in depth how a hash table works but i think it gives a great introduction just one page so that you kind of get a very high level and basic understanding of what a hash table is and this is very similar with everything else it's like there's a section in graph we've seen graphs are the flexible data structure that use nodes and edges to store information they're widely used to represent data like social networks nodes are people edges are friendship relationships telephone networks nodes are telephones and stations edges are communications and much more that's it and then the next topic goes for searching in-depth searching in graph via depth research we keep following edges going deeper and deeper and then they illustrate that via a nice graph again to illustrate my point you get the idea but you're not going to get a full understanding final comment on this book is just the size itself it's so nice and light and compact easy to carry around then you can literally throw it in your backpack especially if you're a student and when you get your free time or break you just pull it out read it or if you're traveling probably not right now during covet but you know regular travel you can just carry it with you you know just refresh some topics you know it almost makes a very serious topic like data structures and algorithms kind of fun with little colorful diagrams and just the size of the book so highly recommend it especially for beginners so once you've graduated from that book you want to go ahead and get this book called rocking algorithms this book also follows a very similar pattern of being very lighthearted funny lots of diagrams to illustrate the point but goes in more depth compared to that book so once you've read that book and you've sort of have a vocabulary of words you've learned that there's thing called hash tables that exist you've learned that there's graphs and how they represent relationship between things and you can search through them via depth first or breadth first and you can do certain algorithms on them but you have no clue how to do them right like that book doesn't give you any algorithms like actual pseudo code or any code it's just an understanding and overview this is very similar but it goes into a little bit more detail and then the table of contents for this book is very similar you'll still get a bigger analysis you'll still get the sorting algorithms the basic data structures greedy algorithms searching dynamic programming that kind of stuff but because you can see it's thicker it's larger it does go into more depth it's like if that's level one this is gonna go level two i'm gonna focus on the same example for each of these books so you kind of get how in-depth they go so let's look at what this book has to say about hash tables in the other book you've learned that hash tables can give you constant time lookups and sort of how hash function makes that happen is magic right in this book however they talk about hash functions what is a hash function how does it store things and then there are a bunch of examples really nice and then it talks about like time complexity how a performance of hash table is not guaranteed right like if your hash function is bad or not efficient then you're not going to get a constant time lookup and then at this point you're like wait but the other book told me that it sort of magically happens right like it happens for built-in hash functions and stuff but now you want to understand what the magic behind it is this book's going to explain you that and then it also talks about collisions that's what happens when a hash function tries to insert two different values to the same spot that's when collision happens and then it sort of introduces that and then it also talks about when the hash function can resize and that's called maintaining a load factor see look those things you didn't know in that book but now you're gonna understand in this book right and again let me re reiterate that this is also a very light read it doesn't have any code it doesn't give you pseudo code it just gives you very illustrated examples and lots of graphs and diagrams to show you or to help you understand the idea even better than that book so i think it's a very natural progression to go from that book to this book and still till now you can go through these books without having much data structures and algorithms experience you don't need that much so even if you are like in school and you just started taking like the basic classes and you wanted to go a step ahead and kind of like get started early these are great books to get started or if you're into like mechanical engineering or different sort of engineering you want to get to coding or you you're thinking about prepping for a big fan company interview like top tech company interview and you don't know what sort of topic you need to cover these two books will sort of give you an idea and you can gauge your understanding you know these may come very naturally to some people and these books do help in that they don't talk about it in the technical jargon that most books do and it's very natural and they give real life examples so it's kind of help you grasp the topic so i think these are this is a great second book to have and then once you've figured that out maybe you've done your own research you've gone online you've looked at it in a little bit of depth and you kind of wrote some code you're you're in school and now you've taken the data structures and algorithms class that's when you bring in the big boy and this is this is a big one and it's it's almost like the bible of algorithms book it's called introduction to algorithms this as everything again those two books have same topic but in this book you will see much more technical detail it will cover everything you left pseudo code you love your mathematical analyses and everything you really need so let's look at the same thing on this guy all right in this case now we're talking about time complexities to begin with even in the first paragraph itself it starts talking about collisions but then it talks about how do you solve collisions and there's a technique called chaining it talks about that it'll give you much everything is in mathematical notation here and there are theorems and proofs and how you can get amortized constant time or you know like uh upper bound and a lower bound to your time complexity is very mathematical at this level if you're understanding algorithms and if you've gotten this far you're you're good to go you know same with graphs representations of a graph we can choose between two standard ways to represent a graph where graph is a set of vertices and edges as a collection of adjacency list as an n or an adjacency matrix either way applies to both directed and undirected graph because the adjacency list representation provides a compact way to represent sparse graph those where the edges are much lesser than the vertices and usually that's the method of choice as you can see it didn't may take any effort to explain like how oh graphs are like social networks and telephone communications that kind of stuff it's going directly to the mathematical and very uh technical uh lingo here but if you have graduated from those two books i think this is the book that you eventually want to get to because this will give you like the real implementation details and thorough understanding of algorithms but i do want to add a note here that you don't really need to know everything in this book like some of it is super mathematical you don't get carried away by this and that's also the reason that i did not recommend this book as number one because if you've never really done data structures and algorithms you're just starting and you take a look at things in this book you will you'll quit computer science i'll i'll promise you that but trust me once you once you go through the other two books and you have a good understanding of what algorithms are and what sort of things they do maybe like brush up on some math like both those book covers like some discrete math and probability counting that kind of stuff you'll find this uh much more approachable and then the final book i had for you is this is this ties very directly to technical interviews not just like learning as a software engineer it's called elements of programming interviews um this is a great book mine is an older edition i think it's like eight or ten years old so it's in c plus plus and you can get this um in java and python these days so i'll link both of those in the description below but this is a very technical interview question first approach compared to the other two books so this is a great one to either use side by side as you're learning topics or maybe go through all of them and once you feel comfortable start kind of looking into this and the chapters here are organized in a very similar approach because like like i said like the topics are same right like so you'll talk about hash table it's gonna give you a brief introduction of what a hash table is but after that it's just gonna go to a problem right like for example design a hash function for a chess game to maintain the states your function should take the state and the hash code for the state and the move and efficiently compute the hash code for the updated state so this is like a real practical implementation of hash code or a usage of ash code right away right so the other question is let s be an array of strings write a function that finds the closest pair of equal entries so not only are you learning the data structures but this will actually give you real life examples on how you can use that data structure or an algorithm to solve interview type questions i know a lot of you guys also have lead codes so maybe lead code and this is unnecessary but if you think of it lead code is like 160 a year so that's more than 10 bucks a month you know but this is going to be a like 30 40 bucks and honestly like if you can solve almost all questions on this book you don't need anything else like this not only gives you thorough explanations of how this works but it's written from engineering leaders from all of the big companies you know so i highly recommend this book so those are the three uh four books that i recommend for getting better at data structures and algorithms one final note that i wanted to add is i do suggest that you at least have an understanding of basic constructs of programming and very elementary data structures like what is an array what is a string what are primitive types what is an int what is a float how do you you know initialize variables or how do you write an if else condition how do you write a for loop while loop that's it that's that if you know that much then you'll start understanding what these are but if you if not maybe the first two books are still approachable but after that um you you'll get in trouble but um and for those um there are many free resources online that you can learn or if you're in school they're already probably teaching you but i will share a couple of other courses that are from uh really good universities and they're free on youtube so you can kind of watch those videos together with these readings and that should help you out a lot that's it if you have any questions write them in the comment below i'll try to respond to each of them if you have a question that pertains specifically to you then reach out to me on instagram and dm me and i try to respond to those too so until next time see ya bye [Music]"
    },
    {
        "video_id": "DFpWCl_49i0",
        "title": "INTRODUCTION TO DATA STRUCTURES",
        "url": "https://www.youtube.com/watch?v=DFpWCl_49i0",
        "duration": 509,
        "transcript": "[Music] hi friends so from today i am planning to upload the videos on data structures so in today's session we will see the introduction of a data structures that means what is a data structure and what are the different concepts covered in the this data structures right right so data structures what is meant by a data structure so the name itself indicates data structure so that means organizing the data in the memory is called a data structure so there are different ways to organize the data in the structure so one example we have seen in the c language that is errors so error is also a collection of elements that means a collection of memory locations so because in the memory locations we are going to store the values right so in that the structure of data is a sequential that means one after another right so it it occupies the contiguous memory locations so it is a collection of elements in that contiguous memory locations so that way the data is organized using the arrays data structure so finally the definition of data structure means organizing the data in the memory location so how many ways we can organize the data in the memory location right now let us see how many types of data structures so there are two types of data structures so what is primitive data structure and another one is non-primitive data structure primitive data structures and non-primitive data structures coming to this primitive data structures we have already seen those data structures see so these primitive data structures are our primitive data types yes so int float character double and pointer so all these comes under primitive data structure non-primitive data structure now so these these we have studied in c language this we call them as primitive data types right integer float character double and pointer it's a single memory location which can hold the value only next coming to this non-primitive it's again divided into two types that is linear data structures and non-linear data structures linear and non-linear so linear means a sequential that is nothing but a sequential so nonlinear which is not a sequential that we can call it as a random random right so this is a sequential this is random that means in this all the data will be arranged in sequential manner here the data is arranged in random manner now what are the types of linear data structure so first one is arrays lists stacks queues these four comes under linear data structure non-linear data structures only two categories are there one is a trees and graphs trees and graphs so again so this we have seen in c language arrays the sequential elements right so now we have to see about the list stack queue trees and graphs so how the data is arranged in list how the data is arranged in stack how the data is organized and used how the data is organized and traced and similarly graphs so these are the different types of data structures right so in the list there are a number of categories so under this one is single linked list double linked list circular linked list allow means lingualist right so here in trees we are going to discuss about binary trees binary search trees and there are a number of trees right so we will discuss in those sessions right so finally the data structure introduction is the same so primitive and non-primitive right and now there are common operations can be performed on these data structures operations so the major operations which can be done is searching and sorting so we can search an element in the data structure right so in the in the list we can uh sort the elements and then see we can insert the new element right insertion similarly deletion we can delete one element next updation so we can update the elements so that means we can replace one element with another element right so these are the common operations to be performed in these data structures so we can search an element in linked list we can sort the elements of a linked list we can insert an element into the linkedin list we can delete the element from the linked list and we can update the element in the linked list so these are the common operations so i will write here common operations on data structures so this is the common operations on data structures right so we are going to discuss about the list in the list we have to apply all these operations and then we have to see the stacks and we have to apply all these operations we have to cover the queues and all these operations similarly trees and graphs right so hope you you got an idea about these data structures and definitely i am going to upload the videos on all these concepts right right so let us stop here so from the next session we will start with the errors so our errors are already seen in our c sections so we will see the advantage and disadvantages of arrays right and then we will move on to the list in the list we have to see all these three so in the coming sessions we will cover all these concepts right so hope you got an idea about this introduction of data structures and if you really understood my sessions like my sessions share my sessions with your friends and don't forget to subscribe to our channel and please share my sessions with your friends and let them do subscribe to our channel"
    },
    {
        "video_id": "iZmDcfTtcNg",
        "title": "What are Data Structures?",
        "url": "https://www.youtube.com/watch?v=iZmDcfTtcNg",
        "duration": 427,
        "transcript": "what are data structures the reason we are talking about this is because if you look at the internet everyone says if you want to get into this company or that company they will ask you a question Based on data structures and algorithms so first of all why such a hype and what exactly data structures are so before we get into data structures let's talk about data see the entire software industry is based on data when you do a course on ID which we call information technology it's not about technology it's about information because we work with information so data is everything so it doesn't matter what technology you learn you basically learn or you work on it so that you can work with data example think about programming languages why do we use programming languages to process the data why do we use database to store the data why do we use AI to generate data or to understand data right I mean just to bring it down everything is data now then question arise what is data structures now if you talk about any programming language we have something called primitive data types if you want to store a number we store that in integer if you want to store a a text you store that in a string if you want to store a character you store that in a character of course depend upon different languages the term or the way you store it changes but ultimately you have some types to it where you store the data but what if you have bunch of data and if you want to store them it's not just about storing data anyone can do that it's about how do you store data efficiently and you can also save some memory the thing is if you simply dump the data you are expanding the memory and if you want to search something from the data will be difficult and that's why storing that data efficiently is very important and that's where data structures comes into picture so data structure is a way to organize and store data efficiently okay so when you say efficiently it means two things first in in terms of performance and also in terms of the memory now what about performance now think about this if you if you talk about any software application which we use it can be a normal calculator or it can be an application like Amazon website or any application which we use a banking application as well now what we do in that application is we use some features using which you can compute something you can process something example on calculator you calculate on the banking website you transfer money so what you're doing is you are basically building an application which will do some processing and the way you build an application is through algorithms now what algorithms set of instructions let's say if I want to add two numbers it's very simple you what you do is you say take two values from the user then perform the operation and give the value back to the user now those are the steps right those are the instructions now what I have mentioned the instructions those are called pseudocode because we're not actually typing a code here we're not being specific to a language let's say C C plus plus Java python JavaScript what we are simply saying is these are the steps you have to follow and that's your algorithm right but yes when you want to make it work you have to convert that into a code which was run on the computer right that's your actual software code and you can use any language as a matter right but the pseudo code will remain same now when it comes to processing of data for any task it's important for us to make it fast and also save memory now most of the companies are focusing on this concept of data structures is because they want to give a good experience to the user of course right if I'm using some application I want it to be fast now you will say okay to make the system faster you can increase the CPU speed you can increase the amount of ram you can do that but what if with the same amount of memory same amount of CPU power you can still make it more faster right and that can be done with the help of data structures now if you know how do you store that data efficiently in a proper structure and by doing that you can make your application faster because in data structures we have different type of data structures example let's say if you have bunch of data you can store that in an array but apart from Adder we have other types as well we have set we have linked list so when to use what it's not that this is best or that is best it's it's all about when to use what and to understand when to use what you have to first understand what those things are right how do you decide that this time you have to use added this time you have to use set and that's where understanding these concepts are very important now these are not the only options we have we have tree we have graph when to use them so when you understand those Concepts then we can think about okay for this situation we will use this and this is why companies are preferring candidates who knows DSA it will help them in multiple ways first it helps them to reduce the cost is because every computation see uh I know you may be thinking most of the applications which we use are free right think about Instagram now when we use Instagram of course we are not paying for it but then companies are paying for it right so the meta is paying for for you to use Instagram so because those computations will be happening somewhere maybe meta is using some cloud service let's say Amazon is in this case So Meta is basically paying to Amazon for every computation which you do okay of course they earn from ads but then they are paying for it so what they will do is they will try to optimize it they will say okay if one query takes let's say one dollar or maybe half a dollar can we just reduce it more can we just make it 10 cents so that's the thing they are trying to do and the way you can do that is by making sure you use a proper algorithm with a proper data structure so why companies are doing it to reduce the cost second to give a better customer experience so that it will run faster and if I search something it should be faster for me as well so as a customer next when companies want to hire people they have so many candidates right how will they filter those candidates now data structures algorithm becomes one of the way to filter the candidates because if you know data structures that means you have worked a lot on the particular language and you understand how how a particular system works on the other hand data structures are not the only thing you need to know if you want to be a good developer if you want to get hired there are multiple things needed example you need to have a good hold on a particular language a particular technology you should have worked on few projects understanding the entire ecosystem not just one language working with databases working with networking but then DSA becomes one of the important thing there so just to summarize what are data structures so data structures is basically a way to organize and store your data in the efficient way and there are multiple data structures options available there and you will understand that in the upcoming sessions we're going to talk about in the upcoming videos you will we will talk about what are the types of data structures we have how to use them when to use what and what algorithms available there so I hope you got some idea regarding data structures so of course entire series is important to understand that properly so I hope you're excited see you in the next video"
    },
    {
        "video_id": "ouipSd_5ivQ",
        "title": "10 Key Data Structures We Use Every Day",
        "url": "https://www.youtube.com/watch?v=ouipSd_5ivQ",
        "duration": 523,
        "transcript": "[Music] in this video we discuss a topic That's essential to every software developer data structures we use them every day and they play a critical role in building efficient systems so let's Dive Right In and take a closer look at some common examples let's start by discussing lists lists are a versatile and essential data structure in software development they are great for storing and manipulating order data they are useful in various applications like task management social media feeds and shopping carts in a task management application a list can be used to store and organized tasks for each user task can be added removed or reordered easily and user can mark them as complete as needed lists are also useful in social media applications like Twitter where they can store and display a users feed in real time ensuring the latest content is shown in the correct order arrays are another fundamental data structure they provide a fix siiz order collection of elements they're particularly well suited for situations where the size of the collection is known or doesn't change frequently arrays are commonly used in mathematical operations storing large data sets or when there's a need for random access to elements for example in a weather application an array could be used to store temperature readings for a specific location over a defined period this allows for easy calculations like averages and trans arrays are also widely used in image processing where each pixel's color data can be represented in a two-dimensional array it enables efficient manipulation and transform of the image next we have stacks Stacks follow the last in first out principle they are perfect for supporting undo and redo operations in text editors or maintaining browsing history in web browsers in a text editor a stack can be used to store each change made to the text making it simple to revert to a previous state when the user triggers an undo operation qes operate on a first in first out basis they are good for managing printer jobs sending user actions in games or handling messages in chat applications in chat applications a q can be used to store incoming messages in the order they are received it ensures that they are displayed to the recipient in the correct sequence heaps on the other hand are used for task scheduling and memory management they're especially helpful in implementing priority cues where we need to access the highest or lowest priority item efficiently trees organize data hierarchically they are useful for representing data with natural hierarchies or relationships they can be used in various applications like database indexing AI decision making or file systems in AI decision making trees like decision trees are used in machine learning for classification tasks trees are also used in database indexing where they can help speed up search insert or delete operations for example B trees and B+ trees are commonly used in relational databases to efficiently manage and index large amounts of data hash tables for efficient data lookup insertion and deletion they use a hash function to map keys to their corresponding storage locations it enables constant time access to the store values hash tables are widely used in various applications such as search engines caching systems and programming language interpreters or compilers in search engines hash tables can be used to store and quickly retrieve index data based on keywords this provides fast and relevant search results caching systems may use hash tables to store and manage cache data it allows for Rapid access to frequently requested resources and improves overall system performance another example is the implementation of symbol tables in programming language interpretors or compilers hash tables can be used to efficiently manage and look up variables functions and other symbols defined in the source code suffix trees are specialized for searching strings in documents this makes them perfect for text editors and search algorithms in a search engine a suffix tree can be used to efficiently locate all occurrences of a search term within a large Corpus of text graphs are all about tracking relationships and finding paths this makes them invaluable in social networks recommendation engines and path finding algorithms in a social network a graph can be used to represent the connections between users it enables features like friend suggestions or analyzing Network Trends R trees are good at finding nearest neighbors they are crucial for mapping apps and geolocation services in a mapping application all trees can be used to store spatial data such as points of interest this enables efficient queries to find the nearest locations based on the users's current positions now let's discuss cach friendless and how it relates to various data structures including lists arrays and other mentioned earlier in the video CPU cach is a small fast memory between the main memory and the CPU it stores recently accessed data and instructions so the CPU can access them quickly without fetching them from the slower main memory now different data structures have varying levels of cach friendliness based on how their elements are store in memory contiguous memory storage like that in arrays allow for better cach locality and fewer cach misses resulting in improved performance when an array element is accessed the Cache can prefetch and store nearby elements anticipating that they might be accessed soon on the other hand data structures with non-contiguous memory storage like link list can experience more cach misses and reduce performance in the link list elements that story in notes scatter throughout the memory and each note contains a pointer to the next note in the sequence this makes it difficult for the CPU to predict and load the next Noe before it's needed the other data structures such as trees hash tables and graphs also have varying degrees of cash friendliness based on the implementation and use case now this disparity in Access times can lead to Performance issues in modern Computing particularly in situations where cash misses occur frequently we should be mindful of this when working with performance critical application and choose the appropriate data structure based on the specific requirements and constraints of the projects and there you have it these are just some of the many data structures we use every day as software developers understanding and mastering these data structures will help us build more efficient systems making us better at our craft if you like our videos you may like our system design newsletter as well it covers topics and Trends in large scale system design trusted by 300,000 readers subscribed at blog. byby go.com"
    },
    {
        "video_id": "SFEROgwxicA",
        "title": "Data Structures Explained for Beginners - How I Wish I was Taught",
        "url": "https://www.youtube.com/watch?v=SFEROgwxicA",
        "duration": 1026,
        "transcript": "for most people studying data structures and algorithms is not the most exciting part of programming and trust me this is exactly the feeling that i used to have when i first started learning about data structures and algorithms to me they just seemed so boring and i just couldn't quite understand why they are even important and this seemed like something that i just sort of have to learn to get my foot through the door and after that i can just forget about them and focus on what i'm actually excited about like coding tinderbots but now that i've actually properly learned about these topics and taking the time to actually understand not only what they are but why they are important i've actually completely changed my mind i think data reaction algorithms is one of the most beautiful parts of programming and computer science and in this video i want to give you a glimpse of this beauty by giving you literally the dumbest most simple explanation of what data structures are to give you sort of an intuitive understanding that you can then take into your actual study of each particular data structure so you can have this sort of framework in your mind going into it and hopefully hopefully you may even start to appreciate the beauty that implementing data structures can actually have and believe me i remember exactly what it was like to not understand anything about data structures this video is completely language agnostic so whichever programming language you've learned in the past you can watch this video and then at the end as always i will give you the exact resources that i use in a step-by-step way to go from this conceptual high-level understanding into mastering all of these topics i'm really passionate about this reaction algorithms so if you enjoyed this video there will be a very similar video coming on algorithms as well so if you do enjoy this video leave a like down below because how many people like this video will then sort of tell me whether i should make it or not okay so what are data structures at an extremely high level all a data structure is is a way of organizing data so whenever you're writing a program the purpose why computers exist in the first place is that we have some data which can be like numbers or maybe it's a string and then we have something that we want to do with that data to produce some useful result it turns out that it really matters how we actually organize data in a computer's memory and i know that right now now this makes any sense and that is why i have organized a couple of very specific examples which will allow you to actually understand it again in the dumbest way possible in a way that even a literal dumbass like me could understand how this work but first let's just briefly talk about why this idea of organizing data is actually so important to the point where if you want to get a job studying data structures and algorithms is literally the most important thing that you need to know if you're good at data structures and algorithms you will be good at writing programs in a logical way in a way that makes sense in a way that is efficient and what companies want to see is that you have that fundamental knowledge and those principles that you can apply in their production code to make the gold really good and therefore that you as the programmer are worth the money that the company is spending on you so data structure is a way of organizing data in the computer's memory the way the computer's memory works in again a really dumb sort of simplified way you have these memory blocks which are called like memory registers all these registers hold some kind of value maybe this is a two this is a five this is a seven and whenever you're writing a program you're probably not just doing something with one piece of data like these two here often you'll have something like a collection of data that's sort of related to each other in some way maybe it's a list of numbers like a list of transactions that you want to add together or something like that so it makes sense to actually organize this data close to each other in the computer's memory so that it's easy for you to access all these different pieces of data rather than having to define separate variables like a equals two or b equals five you can just define one variable one data structure called a list and this is the first data structure that we learned the most basic one and then you put all of these numbers into this one variable so then in the future if you want to access the middle element of this list you can just go list one from the computer's perspective when all of these variables are stored close together to each other accessing them together and maybe looping over them or something is a lot easier this is sort of the computer equivalent of let's say like when i'm making these youtube videos right there's multiple pieces of gear that i always sort of need and so whenever i want to start filming a video it makes sense if i've sort of placed all my camera gear in the same part of the room so all i have to do is go to that part of the room grab the gear and start filming and storing values in something like a list is just a computer equivalent of this same principle let's talk about a limitation that a very simple data structure like a list could have the way lists or arrays are usually defined in a programming language is there a certain amount of memory in this case we've just allocated three registers of memory to our list here but what happens if we now want to add a fourth element to the list well you might say that well that's easy we just sort of added here but what if this register in the memory was already taken up by a different piece of data maybe we already had a string like hello in here now we put the eight on top of it so this hello gets replaced and maybe we had a different part of the programmer we'll be using that piece of string and now instead of hello it's certainly an eight and just everything crashes right so the way lists are usually actually defined under the hood in the language is that if you add an element to a list it will actually take this list and move it to a completely different part of the memory and then allocate more memory to it even if you don't understand anything about low level programming you might see that this is sort of very inefficient if you know that you'll be constantly adding data to a list every time moving into a different location in the memory can become very slow so that is why you can see that actually if we know that the thing we want to do with our data is to keep adding stuff to it and we know that in the future we'll be adding a lot of stuff to it it actually doesn't make sense to use a list and there might be a better way to organize that data again for this purpose that we have here and in particular for something like this we might use something called a linked list and what a linked list is instead of storing your items just sequentially after each other in the memory what we're doing is we're defining this node structure so we just have two places in memory where the first one is a piece of data like r2 here and the next one is a pointer to a different node somewhere else in the memory it doesn't matter where it is essentially this second item of this node will be pointing to the next element which will again just be a node and the way this actually works is that these memory registers are numbered in the computer's memory let's say this is a number five is number six and then somewhere randomly in the memory we have a memory register i don't know like 102 and here as the second element of this first node we just have the memory address where we know that the second element will be and again for the next one we would have maybe a pointer to register 463 and this one will then know that okay the next element of our link deletion will be in memory location 463 and again using our camera example here let's say i'm using all my camera gear in this part of the room here but now there's no space anymore what i could do is include like a post-it note in that area to see that okay the next batch of my gear is going to be in this area of my room maybe there's a note there that says i don't know under the bed or something so that i know where to go to find the next part of the gears and this way you can essentially just keep increasing this chain of values or chain of camera gear almost indefinitely as long as you actually have a space in your memory or space in your room all you have to do is just know where the very last element of the list is and then users have to find an empty location anywhere in your memory and then add another node into your linked list and so you can see how we already with this very simple requirements we already sort of have a need for a slightly more complex data structure and from the programmer's perspective when you want to add something to the list you just call the linked list sort of a method that's defined inside of the data structure to append an element in the list and under the hood all of this stuff is happening all this sort of drama is happening but the programming doesn't even need to know anything about it but crucially if the designer of this program hadn't designed this in the correct way it could be causing problems like this and causing the whole hardware to essentially crash or explode or whatever horrible things would happen but again obviously even the linked list also has downsides let's say what's really important in the specific application we're developing is for example accessing elements in the middle of the list like accessing this one right here now this specific operation is actually a lot easier to do in an array or a list like data structure we just call list one like this here whereas with a link to this we just sort of have to loop over the list until we arrive at the item that we're looking for which is a lot more expensive so as you can see again which data structure we're using always depends on the specific thing we're trying to do and sort of the specific things that you can see the user of that program needing to do a lot of the time and the way all of this like the linked list would be implemented in an actual programming language or python is that you would do something like create a class called linked list then you would create method inside that do all of this magic and then there will be a client where all they would have to do is call this append method and it would simply just do all of this magic behind the scenes and that is really the beauty of data structures and i know that all the details of this might be a bit fuzzy for you again the point of this video is not so that you can understand all these details it's just to give you an idea of why different data structures even exist if my sort of weird way of explaining data structures is making sense to you at all i would appreciate it if you could tap the like button down below in the description so let's now move on to a couple of slightly more complex examples to really drill this in and to make sure that you really understand what's going on here but first a word from our sponsor if you have a business or you're working on a project that requires e-signature you want to make sure that your documents are signed securely fast and in a legally binding way so if that's you you need to listen the sign now api is a powerful e-signature tool that allows you to embed e-signature on your website it's free to test fast to deploy and will allow you to not worry about document approval cycles and instead focus your time on growing your business developers particularly love site now's easy to use and detailed documentation with clear coding examples as well as great video tutorials on how to get started which makes implementing the api extremely easy in any programming language the synonym api is available through straightforward sdks and allows you to complete entire document approval cycles from uploading documents to tracking signature progress and not only that design now api make sure that your apps keep running with 99.99 uptime and in accordance with all the required compliance and security requirements so if that is something that sounds useful for your app you are in luck because they currently have a unique offer on the market which gives you 250 legally binding e-signatures for free so if you don't want to miss out on that click the link down below in the description to create your free sandbox account and start building dynamic e-signature workflows today thank you for sign now for sponsoring this video and now back to the tutorial so in practice we obviously have a lot more even more complicated requirements for things we want to do with data let's say you are an airline and you want to sort of structure the data about all the people who have bought tickets to some flights what makes sense for you is probably not just to group stuff together randomly in the computer's memory like for example in the order that they bought the tickets because what you probably really want to do is order them in some sort of priority where the people who have bought first class tickets are first but then when they want to do something with it they can see the first class passengers first and then the business class passengers and then the peasant economy passengers like me so it would be very useful for these airline operators to have a data structure that would allow them to group data in a way like this and it turns out this is a very common requirement in many different computer programs that is why a very common data structure that is used is something called a priority queue for this you would define essentially a linked list just like this but essentially every time when you're adding something to the list but the add method wouldn't just be doing this it wouldn't just be adding it to the end of the list but rather every time we're adding a person it would do some sort of operations which automatically sort of orders the list in the way that the airline wants so it orders it in a priority order so here what we would probably have is instead of just a piece of data and the link to the next node in the list we would have also a field for the priority class of the passenger so we would have a node that looks something like this where there's three values there's the actual person probably this would already be something like a dictionary with like their name and their birthday the price they paid or whatever we would just have a value that indicates that this is a business class passenger and here again we would have the pointers or the memory address of the next person on the list and then every time we're adding a person we would have defined inside the data structures of app operations that automatically perhaps it calls a different function to reorder all the passengers in such a way that this passenger will entity do the correct location of the list so as you can see a lot of the time even though you might think about data structures it's just about the structure so where we put the data a lot of the time especially these more complex data structures are a lot more about the operations that we want to do with that data and that is really the other thing that you should keep in mind that when you're defining data structures is really not just the organization of the data but also the operations that we define on that data and again the point here is that the airline operators or the person who's like going through the ticket doesn't need to know anything about any of this drama that's happening down there in the computer memory because the programmer the programmer who understands data structures has been able to program it is in an intelligent way from the airline operator's perspective all this is just happening like magic when they add a new ticket into the pool of ticket holders the list is automatically organized in the exact priority that they want and that is again the magic of data structures and this whole principle is also something that we often refer to as abstraction the most beautiful thing about computers and computer science is that there's like a million different levels of abstraction where at the very low level you just have zeros and ones inside the computers or even lower these teran sisters where electricity is going through these logic gates but you don't need to know about any of these because there are low level programmers who have designed these higher level programming languages that allow us to do stuff with the low level computer without understanding all the drama that's happening down below and the same thing here when as a programmer as a high-level program and your most important task is to understand the language and to understand programming concepts like data structures in such a way that you can design programs so that the people who use these programs don't need to know anything about how they work thinking just use them you just tell them click this button or run this function and it will do exactly what they want and in reality the relationships with pieces of data and the things we want to do are even more complicated than this where for example if you're organizing web page like the way google actually organizes their web page they have these like very deep complicated relationships with like all the different web pages so they need structures to deal with it again for the purposes that they need the data for so at the end of the day the reason why companies care about all of this the reason why you need to know all of this as a programmer it's like the equivalent of someone who organizes their room like a complete mess no one wants to hire someone who doesn't know how to organize their work effectively and to do stuff in an efficient way and so this is the analogy of the programmer who just does stuff in a way that maybe works but it's not the most efficient way to do it and for companies especially these large enterprises where all these details really matter because they're dealing with like billions of pieces of data they want to hire programmers who can organize their code effectively and that is really what data structures are all about if you want to actually now learn these details obviously this video is not enough first of all i would recommend subscribing to this channel because again if enough of you like this video i want to make a similar video about the other side of these virtual algorithms which is algorithms but really to learn all of this is not easy and to learn it effectively you need a good step-by-step plan and a path to make sure that you're learning everything that you need to know in the most efficient way if possible and for that i can be for you i mean this video where i talk about exactly that i describe exactly an exact step-by-step plan that you can take from a high level overview into all the theoretical details if you want to master all these topics and learn to pass coding interviews you should absolutely watch this video right here"
    },
    {
        "video_id": "wy0TPi9M7VM",
        "title": "Abstract Data Types | DSA",
        "url": "https://www.youtube.com/watch?v=wy0TPi9M7VM",
        "duration": 440,
        "transcript": "welcome back aliens my name is David ready and in this video we'll talk about ADT which is abstract data type now before we move towards ADT let's talk about data here of course in the previous video when we talked about what are data structures we have talked about data and we also mentioned that in the software industry everything is about data so whatever you do you are doing it for data right now with this data basically you get data from the user you process data you also give the output to the user or maybe you want to store this data in the database for the permanent storage but the thing is it's all about data and whenever we use any language doesn't matter which language you use what you do is you store that data in your code somewhere and to store that data we use something called a variable now imagine variable as a box and you are keeping your data inside that box the problem is every box need to have a type of course there are languages which are dynamically typed language where you don't don't actually mention the type of the variable but in general this box will have a type so that means if you want to store the data you need a box which is your variable and this variable also needs a type to it or this box needs a type to it now example let's say if you want to store a number so we can use something called an integer or if you have any other point values you can also use float or double now it depends about the languages how they work but in most of the languages we have something in common we have integer for the normal numbers we have a float for the point values and what if you want to store a text that's why we have string and in few languages we don't have all these types we have limited types and in other languages we have many but that doesn't matter right in general we'll be having a variable and a type to it now all this type are called data types and they are system defined data types or you can also call them as primitive types is because it's there in the language itself you can directly use them but sometimes you want to use some complex data type and we can create that with the help of some other Concepts example let's say if you want to represent a human or if you want to represent a phone now I love phone so I'm I'll be using that example so if we talk about this phone here now this phone will have a name to it right so it will have a name it will have a brand of course name is model number then brand then the configuration the amount of CPU Ram all this are your data right so if you want to represent any physical thing in the virtual world we use objects there right now in some languages which are object-oriented programming we do that with help of objects example in C we use something called structures so we can define a structure name let's say phone and inside that you can specify what are the properties there in the same way in the object limited languages we use something called class in this class you create different variables with primitive type and then the Box itself or the class itself is your complex data type or you can say user defined data type user defined because system is not giving you we are defining our own type right so that's about data and data types but let's say if you want to store a bunch of data how will you work with that and when it comes to data structure how will you store this data in a proper way see what happens is when you talk about primitive types as well every type will have a way to store data of course that's why we are getting them right so example if you are creating a variable called num now let's say this num is of type integer and then you are storing a value to it now with this variable num you can perform operations as well now what are the options you can do with integer you can add two values you can subtract two values you can divide multiply so you can perform those operations on the number but let's say you have a string there now which string of course you will not be adding two strings that doesn't make sense right why you will add two names let's say then you will get a new name okay a lot of parents are doing that for their kids but that's not the idea here right we can't add a string we cannot subtract strings but yes which string you can do something else maybe you want to see the size of a string maybe you want to concatenate two strings maybe you want to cut two strings so string has a different operations in the same way if you are creating your own type this will have data of course but also you have to you have to mention the operations which are going to perform so we have two things right we have data and we have operations now let's say in data structures you want to store a bunch of data as we mentioned and you want to store it so there's a concept of array so if you have multiple data instead of showing them in multiple variables let's say you have five numbers uh 2 6 12 21 and so on now if you have all these values you basically store that in five different variables or you can create a single array and you can store all the values there the advantage would be you have just have to use one variable name there so instead of saying a b c d e you can simply say nums you can use any variable just doesn't matter so what's important is you can use array here now when we use array we are not basically using a primitive type here we are creating our own type here own data type which is an array now in this array basically you should be also able to perform some operation example let's say what if you want to read some value you want to fetch a particular element you want to search the array maybe you want to add an element maybe you want to delete an element so this operation should be possible on that type so in the concept way when you have a type where you can perform some operation we call them as abstract is because the implementation for array changes from language to language and we don't just have added right so let's say if you want to store a bunch of values we can use list we can use set queue now when you talk about a queue here let's say now inside Q we have I mean we have one more option for that which is stack they follow different concepts so let's say if you want to add element in the queue which follows last in first out which means you have to insert from the end and you will get it from the first of course you can reverse it you can insert from the start and you can take it from the end the thing is you have you will insert from one end and you will remove from the other end so that is first in first out in stack it is reverse you do last in first out example let's say uh took an example let's say for Q we can say a queue which you follow right example if you want to buy a coffee from a cafe shop of course there's a cue there you have to stand in a queue and then when your number comes you will get the coffee so basically the first person who went there first will be getting the coffee first right in terms of Stack it is different so when you keep multiple books the first book which you can take out is the last book which you have kept there right it is the last in first out that is your stack now we have different way of implementing in different languages the concept remains same right so when you have a concept and the associated operations to it and of course with data which we call them as abstract data type we also have map but we'll talk about those things later at this point the point remember is we can create our own types and which is a concept and if you want to have data inside that and also you want to specify what operations you can perform that is your abstract data type of course in this subscription videos I will talk about how do you create them how do you work with them and it will be fun so see you in the upcoming videos Everyone bye"
    },
    {
        "video_id": "n0e27Cpc88E",
        "title": "What is Abstract Data Types(ADT) in Data Structures ? | with Example",
        "url": "https://www.youtube.com/watch?v=n0e27Cpc88E",
        "duration": 616,
        "transcript": "yo what's going on guys Thun Meyer for simple snippets and welcome back to another video tutorial under data structures and algorithms and in this video tutorial we're going to be understanding what is an abstract data type in data structures so when you're studying data structures you'll come across this term abstract data types quite often and it's a theoretical concept so we will quickly understand this and the best way to understand this is by taking examples so in this video tutorial we'll understand what exactly is an abstract or logical view and the implementation view of a data structure so that is the two ways in which we can study or view at data structures and we'll understand this by taking an example so with that being said let's get started so what exactly is a abstract data type now we have the definition of data structures which we've already seen in the previous video but I just mentioned it so that you can reiterate it so in computer science of data structure is a way of data organization management and storage that enables efficient access and modification so this is something that we've cleared out in the previous video of this data structures playlist but the way we look at data structures can be categorized into two parts you know or two different types the one is where we actually have a logical or abstract or mathematical view or model wherein we just specify what all things the data structure is going to have now we know that data structure is a way of storing data right so when I'm seeing way it means that there are some protocols there are some rules that are gonna be followed right so all those protocols all those rules can be modeled as a proper view or a model and you can basically just write down all the specifications right so that comes under the logical or abstract or mathematical model or view on the second way or the second part you can say is the implementation part wherein you use all those rules and regulations and actually implement that using some programming language right so Indian obviously we are going to be implementing these data structures in practicals by using some programming language in our case we are going to be using C++ programming right so all these programming syntaxes will be based on this mathematical and logical or abstract model so now that you have a overview of the ways in which you can look at data structures let's try to define what the abstract data type is and let's take those samples into consideration okay so when you are talking about abstract data types ADT is that is the short form are entities there are definitions of data and operations but do not have implementation details so basically the abstract data type is the logical or mathematical or abstract view that we were talking about right so here we have the entities that are definitions of data and operations but do not have implementation which means that we know what we are going to be storing and we also know the operations that can be performed and the way in which the data is going to be stored depending upon what data structure we are going through but we haven't yet implemented it in any practical sense now the reason why we do not have any implementation in edit is because every different programming language has different implementations for example a particular data structure in C can be implemented using the concept of structures but that same data structure can be implemented by using the concept of objects and classes in Java programming and so on and so forth you know so different programming languages have different implementation strategies to tackle different abstract data types so basically data structures are these abstract data types which has specifications about how the data is going to be stored and what are the operations that can be performed on these data types and these different entities but the implementation depends upon the programming language that we use okay so to get this thing more cleared out let's actually take a real-world example so here's a real world example of a smart phone okay so if we were to look at this smart phone in an abstract or logical view what we are going to be doing is we're just going to be defining what all things does a smart phone have at a high level okay for example this smart phone will have 4 GB RAM this smart phone is having a Snapdragon 2.2 gigahertz processor so every smartphone has a ram as processor has a 5.5 inch LCD screen now the screen size obviously varies depending upon what smartphone we're using but this particular smartphone has dual cameras it has Android operating system which is 8.0 version and whatnot you know and along with all these properties this smartphone also has some functionality also has some operations and behaviors right so this is what that operation stands for and this is the data rate so information can be termed as a data of that particular entity we are looking at smartphone so this is all the information and data about that smartphone and the operations are you can use your smart phone to call you can use it to perform text you can send text you can click photos you can click videos and so on and so forth so these come under the behavior and the operations rate so this is what an abstract or logical view of this smartphone looks like wherein you just hitting all the different properties and all the different operations and behaviors that that particular entity can do but when you look at the implementation view over your so when you are actually implementing this in terms of programming in terms of actual code this is how the pool would look like in a proper C++ programming language so you are creating a class so of course we have objects and classes in the C++ programming language if it was C then we could have used structures so in that we have these different variables created follow those respective properties right we have RAM size processor name we have screen size and their respective data types obviously in string float and then we also have some methods which correspond to these behaviors so we avoid call text and obviously the definition can be defined later on but right now I just want to state or show you how the implementation he would look like now if this was some other programming language let's say it was Java or Python obviously syntax is going to be changing some keywords and some mechanism is also going to be changing right which means that the implementation view can change however the abstract and logical view is independent of this implementation so this is what this hashtag datatype actually means now coming to our data structures word let's take an example in the data structures environment ok so let's take the basic integer array so we've seen an array data structure in the previous video also so let's continue with that only so that you'll understand it in a better way now here's an example which has an integer array of size 4 okay so we have the index position starting from 0 1 2 3 so array is a data structure wherein it is a collection of elements which has stored add contiguous memory locations so you can see in the orange we have memory addresses which are just one besides other so one thousand one thousand four one thousand eight and one thousand twelve so this is basically in bytes and each integer element in C++ takes up four bytes so that's why four bytes and then we have 1 0 0 4 and then one 0:08 and the addresses are allocated just right besides each other which means that it is contiguous in nature and not continuous okay we have index positions allocated and then the blue ones are the actual values so this first position is storing value 10 this 20 30 and 40 so this implementation so this is basically what an array is and all these index positions value memory address all these things are implemented in programming languages differently by different programming language right so the abstract or logical view of this integer array can be stated as this data structure or this array store a set of elements of integer datatype so in this case we are using integer array right so you can also say of a particular datatype if you want to go more generic then we need elements by position that is index so we have indexes to access different positions starting from 0 we can again modify elements by its index so if we access one particular index we can change the value at that index and we can also perform sorting and obviously there would be many more things that can be done using this array data structure right now I've just given an example of how I abstract or logical we would look like now coming to the implementation side this is how it will look like in C++ programming so the syntax would be int arr I am creating an array of size Phi in this case and not for I am having one two three four five as the actual element so I am also initializing the array I am saying C out ARR of one which means I want the value at position two okay so this is zero index position this is one index position if I am saying error of one so the output would be two over here now what I am doing is I am saying error of 2 equals to 10 so this is the third position or second index value the value is 3 over here but when I am saying error of 2 equals to 10 the new value will be 10 so I am modifying it by using the index rate so this is what is specified over here so this is basically the implementation view so when we are talking about abstract data types it is just entities which have a definition for how the data is going to be stored and what kind of data is going to be showed and what are the operations that are going to be operating on that particular data so this was an example of array rate so if you go ahead and check out other data structures as we move ahead we will obviously see them but just for example we can talk about stat szostak is a linear data structure which works on last in first out which means the last value being added into the stack will be popped out first or you can also say first in last out which means the first value that goes in will be the last one to come out so that would be written in this logical view and then obviously the implementation is something that we have to type in in form of code depending upon what kind of programming language you use so that's why implementation always keeps on changing a little bit here and there obviously the operation that is the actual behavior is always going to be the same just that since every programming language has its unique properties also the implementation slightly changes okay so this was a little bit detail about what is abstract data types you don't really have to stress out on this topic it is not really a big thing it is just as I mentioned small entities that are having definitions of data in operation but do not have implementation that's about it this is one line that should be enough and as you can see we've also talked about the example so abstract data types are nothing but the specification that we provide without the implementation okay so that's it for this video guys I hope you have got an idea about what are abstract data types you might come across this term quite often as you move I'd so thanks for watching guys if you like this video please share it with your friends do give it a like let me know comments of this video watch and I'll see you guys in the next video peace"
    },
    {
        "video_id": "bR0NYdmMg94",
        "title": "Arrays and Abstract Data Type in Data Structure (With Notes)",
        "url": "https://www.youtube.com/watch?v=bR0NYdmMg94",
        "duration": 1525,
        "transcript": "So guys I have a friend that wanted to build a PC So he approached me and asked \"Harry, can you give me a blueprint to build a PC?\" \"So that with that blueprint I can make someone build a PC?\" \"I want to do gaming and streaming.\" So I told him, \"Do this one thing, buy the motherboard of MSI tomahawk b450,\" \"And along with that, buy an AMD Ryzen 3550 processor,\" \"And with that, don't buy an expensive graphic card;\" \"Use 10-50 TI, okay?\" So I gave him this blueprint. But why am I telling this to all of you? I'm telling this to you because This blueprint of the PC that I gave him With the help of that blueprint, He can go to any store, or by himself He can build a PC. And I told him all the main components of a PC, And even if he makes a mistake, It won't be a big deal. And he won't be able to make a mistake if he follows the blueprint correctly. Okay? So what was here was, I gave him a blueprint Okay? I gave him a blueprint And a...let's say that there were some minimal requirements. That need to be put in a PC, And now, with the help of this, he'll make his PC build. Okay? Now how is PC build related to Abstract Data Type? You might not even know what PC build is, maybe Or maybe you guys are champions, but that's not the point. The point is that I told him which processor to use, I told him which graphic card to use, I told him which motherboard to use, Maybe I'll even tell him the RAM chips; \"Listen use these chips,\" You can use Kingston ones...there are a lot of chips in RAM; let it be. But, I gave him a blueprint, And I said, \"With the help of this blueprint, build a PC.\" Now this Abstract Data Type is... Abstract Data Type is a type of blueprint just like this, That does what? It tells us minimal requirements along with some operations. Okay? So there here there are some operations, And along with that, there are some minimal requirements That the data type will follow. Okay? Here I'll write minimal requirements. So abstract data type is a model To make a data structure. This means that once I give in the abstract data type, Then after seeing the abstract data type, Anyone can write the implementation of this data structure. And they can make a data structure.  And this data structure will be the actual implementation. So abstract data type if I say about arrays So you can implement it in many ways in different programming languages. But I will give you some minimal requirements And some operations on them. So this abstract data type In that, I will give you MRF. Not the MRF tyres, nor the bat one; I'll give you minimal...minimal Required functionalities, okay? And if any of this confuses you a bit, Functionality; okay? This might sound a little confusing Because you guys will ask, \"What is this ADT; how did it come?\" Just understand the way I made this for PC, I gave the minimal required functionality. In this, you can do streaming and gaming; These are some of its functionalities. And along with this, I told you That all of these things should be there. After this, anybody can make it using any cable; They can use any RAM to make it. Many implementations can be made for this. Many PCs can be made using these components. Similarly, what is abstract data type? It tells us some minimal required functionalities, It says, \"I want all of these.\" For example, if you go to a carpenter, And say, \"I want a table. I want a table that is adjustable,\" \"I should be able to code while standing, and I should be able to code while sitting.\" \"The rest is your choice to make anything,\" \"You can use iron, wood, plastic, aluminium; use anything, I'll be fine with it.\" \"But this is the minimum required functionality that I need.\" And some operations, that you can define later on your own. For example, in that table...let us say You give an operation, like...to open a drawer So you'll give an open operation in it, Along with that, you close the drawer, You'll give an operation of that type. Let's understand this with the example of an array. Now we'll talk about array ADT. Let me just clear the board If you guys are taking notes, good for you; But I have made the notes, you should definitely download them from the description.  You all should definitely download the notes. Okay? I'll rub this away now And uh...I'll rub this away And along with that, what will we do? We will study array ADT. Array as an Abstract Data Type. Okay? Arrays as an abstract data type. By the way, what are arrays? Arrays can be found in a lot of programming languages. Arrays can be found in C, C++, and Java; in Python, they are called 'lists'. But, there are some minimal required functionalities in arrays. For example, you might want to be able to so Get, There is a Get method in that And if I write Get (i), then I should get the element for the i-th position I can do any Set; I can set any number at the i-th position. As in I can make the value of index 'i' numb. Okay? Along with that, Get is done, Set is done. Then if I'm making a custom data type If I'm writing an implementation of the array, In that, I can bring in some methods, For example, here I can put in a method, 'Insert', That you can perform on arrays I can put in a method in the array, 'Add' If I want to add two arrays. So there can be many methods like these, It depends on the implementation Okay? But the minimal functionality of the array That you'll be able to Get and be able to Set. Okay? And along with that here there can be a resize functionality All of these are methods, or they are operations Okay? That you can perform on arrays So you can make your custom data type By using this blueprint And it will depend on your implementation. For example, I will show you while making it using a structure But you in C++ you can make it with classes. Now if you're making it with structure, It's your choice; it will depend on your implementation, That when the size of the array is increasing, How will you resize it? If there is a resize method Or are you copying everything? Are you using some other strategy? Are you keeping an auxiliary array already? So these things depend upon the implementation Okay? So here, in the array I have defined the Get and Set; these minimum functionalities For this abstract data type. And along with that, these methods can be various methods There can be various operations that you can find in an array. Okay? Now all these operations can be defined And we can make our own array ADT. Now, what does abstract mean? Abstraction means hiding details. It's very important. What does abstraction mean? In the local language, if I have to explain abstraction, Then it means, eat the mangoes, don't count the kernels. What does abstraction mean? Abstraction means eat the mangoes, don't count the kernels. Meaning, don't go for the implementation details. Don't go for the implementation details. Only go for the usage Please use it. Implementation is hidden. All these things will be hidden We'll say that use Insert and insert it in the array It has nothing to do with you, That we've made a structure inside the insert Or we've made a pointer Or if we've cramped up anything else inside it That's my concern; you guys just use it. Like when you buy a phone, Do you open it and see it? Which screws does it have, and which processor does it have? You read the box, that it has a certain processor But it's not as if you guys open the screws Because the mobile phone manufacturers tell you To talk, use Instagram, use Whatsapp, use Facebook Watch Youtube, watch CodeWithHarry videos Do whatever you want. But all those details are abstracted from you. Now the Youtube app How do you know what code it has? It's abstracted from you, it's hidden. So hiding the details. Abstraction means hiding the details. About what has happened and how. Hiding this is called abstraction. Okay? So this is an abstraction. And abstract data type...I dropped my pen. No worries. So this is an abstract data type. Okay? So we have studied arrays ADTs. Now let's discuss an array; what actually is an array? I'll rub away these methods and all I'll clear the board up. And I'll tell you a bit about what an array actually is. Okay? So an array is a collection of elements. Collection of elements. Accessible by an index. I've given this in the notes; so don't worry too much. Accessible by an index. By an index. I'll write a little fast. Scribbled handwriting. But I've written the notes in perfect handwriting. Don't think that the notes are like this; the notes aren't like this. This is only to make you understand that I'm writing fast. So what is an array? Let's see here. Now I told you that the memory layout of the C programme Here there is a code section. Okay? Then what do we have here? We have the initialise and the uninitialise segments. I won't talk about those. Then we have the stack. Okay? Here we have the heap. Okay? Inside the stack, there are local variations of a function. Okay? So you called for a function; then it will have its own stack. In that, if you make an array then it will be here. And you'll get the memory here, in the stack. But the heap, i.e in the dynamic memory... If we use the memory as a resource in the heap of the array, Assume you say that you want an array of 10 elements, Now here 0...1...2...3. Till will go till 9. And the index of the array In most programming languages, starts at 0 It has a reason; when we start from 0, We get a certain advantage but I won't go there. But, you can search on the internet There's a paper by jee-stars You can see; in that they've explained what will happen if it starts from 1 If it starts from 0; how calculations will become easy When we try to access the index ahead. I won't go there. I want to tell you guys, What an array is. Now assume that here...or here You request an array in the heap or in the stack So you get contiguous blocks of memory. Means, that if its address is 50\nOr if its address is 10, And I'm assuming an integer, It's an integer of 4 bytes. Okay? There is also a 2-byte integer too But I'm assuming a 4-byte one. It depends on the architecture. 14...18...22...26...this is how the address is. This means, that 4 bytes is one integer. Okay? Here assume this is 6. Here in 4 bytes, there will be one integer. Here in 4 bytes, there will be one integer. And there will be an integer every 4 bytes. Now assume that this is 26...so this will become 30. And along with that, this will be 34. And from here, the other memory's location will start. Which I don't have; it's not under my control And that is 38; okay, and from here the other memory will start. Remember one thing, you cannot resize an array. Because if you resize it, then...you want this memory This memory. Now assume you made this array, Okay? You made this array The compiler will say, \"Okay,\" \"From 10 till 38; this block of memory is yours.\" Okay? And you will get it. Now assume that after some time you go to a shopkeeper, \"Uncle please give me this toffee,\" He says, \"Take it,\" Then you go and you think that this isn't a good toffee. Then you say, \"Uncle please take it back,\" Then he asks, \"What's going on? I'm not here to take it back,\" He gets angry. Okay? So what happens here? Goods once sold can't be returned. Okay? So here, if you say that you don't want 38, you want a greater one Then you won't get it. Why won't you get it? See, when you requested a memory till 38, Then it is possible that some other application was given some memory by the compiler. Okay? It will be doing its job with this memory. Otherwise in the same programme It must have allocated a variable in correspondence with this memory. So now you're saying that you want to increase it So you cannot make it larger in this array. You cannot make it larger in this array. You can increase it in the linked list, we'll talk about that ahead, But you cannot make it larger. Because you've asked for memory till 38, After that who knows who's using the memory ahead of that; It won't be used in every case. The compiler can't even guarantee to increase it That, \"I will guarantee that I'll increase it whenever you want or resize it,\" So for resizing it, what you'll have to Assume the content of this array...1...2...8...12 And assume this is 8...36...42 So you'll have to copy all of this content into a new array. If you want to request a larger array. You'll have to copy all of this content one by one. You can resize it this way. Okay? You can resize it this way. Now if you're making the array in a heap, You can make it in C++ If you are then, you can use the new operator If you're making it in C, then you can make it like this: int*malloc. By the way, if you haven't seen my video about C It's a 15-hour video with notes. Do watch that. Because all these things are needed here. I've told these things very slowly with practice If you skip the practice set, it'll be over in 6 hours. Okay? Do watch that video. I've made you practise a lot in that video. That's why it's taken a lot of time...it's of 15 hours. Malloc, and... assume here I'm doing 10xsize of int. I'll write it like this. 10 multiplied by, i.e star, size of, i.e size of the operator, And inside that int, i.e whatever is the size of int. So assume this is an 'a' pointer, okay? This is an int*a. I have an int*a. And I've pointed it with this. Assume I made an array of size 10, So I'll get 0-9 as my indexes. Indices. Okay? And after that, I want to expand this array. And now I want it from 0-19, That means that I want to store 20 elements in my array, okay? So what will happen? To store 20 elements in the array, What will I do? I'll make another pointer. Int*b=int*malloc(20*size of int) I'll do it like this. And then I'll write, a=b. Okay? I'll point this pointer in this. I can resize it in this way. Directly I cannot resize it, I can't increase it It won't get this block; no. It can be resized this way An array cannot be resized Now, why should we use an array? Why did we make an array? Let's see here. We made the array because Because at one point I have these 10 addresses Then you'll say, \"Tell me what the 4th element will be,\" So here I will count and I will know that this is at a distance of 4 So the fourth element; I'll try to find in one calculation i.e in O(1) time. Listen to me carefully. I will use 10 And after that, I'll write + 4x(i) This will be its address Of the i-th integer. Let's see if I've written it correctly. If I do i=0 here, then I'll get 10. If I do i=1 here, then I'll get 14. I'll write here 0...1...2...3...4...5...6. Now assume that I want to know where the 5th element is. I want the address of the fifth element; As in I need 30. What I'll do is; I'll put 5 here 5x4=20; 20+10=30. So I'll directly get the address and I'll be able to access it. So there is faster access in an array. It's not like if I want to access this then I'll have to come from all the way here adding the pointer. So this traversal; if I'd made it with a variable or linked list or in any other way, I would have needed it on O(1). Now my work is done in O(1). Oops. My work is done in O(1); in constant time. I can access the elements of the array in O(1) time; in contact time. Okay? So this is benefitting me a lot; I can access anything faster This Get method runs very fast. This Set method also runs very fast. Because I can reach wherever I want to go. I'll directly say that whatever value is at that place, To dereference it and numb its value. Okay? This way I can change the values very quickly. But here there's a scam. What's the scam? I'll tell you. The scam is that if I want to insert an element here, If here I want to insert 5 after 8; So if there isn't large enough, I'll have to make another one. I'll have to copy these elements here, After that, I'll have to keep the new element that I'm inserting. Then I'll have to copy 36 and 42 here. Okay? Then I'll have to copy 36 and 42 here. I'll have to make a new array. But this insertion is becoming costly for me Along with that, if I want to delete something Assume that I don't want 8 here So this deletion will also be costly for me What will I have to do? I'll have to move these elements a bit. It is possible that I'm deleting the first element And the elements left to move inside the array, Those are 'n'. It will take O(n) time to only move them. Okay? And after that, one place will be wasted. Let's not worry too much about that wastage. But after that you can imagine; I'm moving all the elements ahead of it. That operation there will be costly for me. We'll do this one thing now, We'll write Get and Set directly. The programming language gives them to you by default If you've made the array in C language, You can write it like Arr(i) and implement it in Get. And in Set also you can do it the same; write Arr=(numb). So Get and Set is very easy in the C language. But we'll see a few other operations, We'll implement this array ADT with the help of structure. Because it's our choice, how we build our PC. This is our abstract data type. We'll build it however we want to. And we'll add many more operations So that whenever a user is using the array, then they can do so very easily. And they should not have any problems while using array ADT. For example, if he/she goes to 'Add Array' to add two arrays, Now in C language, you'll have to write whatever function or programme that you have to add. But in my ADT I will provide the user with the ability to use my Add method. So that he/she will be able to add lots of arrays directly. Okay? So we implement this thing with the help of structure. I hope ADT is understood. And that array is understood. That why we use arrays. Faster retrieval, faster updation. Where does the scam happen in the array? At the time of deletion and insertion. If I want to insert an element, it becomes costly. Because I'll have to move it; All of this is contiguous memory, right? All these locations are contiguous. And if you say, \"Send 8 at the end; bring this here,\" So it doesn't work like that. I have to maintain the order, And do the insertion after that. There I will face a problem. And if I need more space, Then for that, I'll have to make another array. I'll have to copy everything in that. So that's why we use an array; because there are certain advantages to that. There are some disadvantages too, but it varies from use case to use case. When I use it; when I don't. It depends from problem to problem. And you'll get that by practising. And I will ensure that you get it. Now here I'd like to say something, These notes for array, I'll give them to you. I've written ADT and array pretty well. Now I'll show you the notes, And along with that, If you haven't accessed my data structures playlist, Then please do so. And also, I'm posting a lot of content on Instagram these days, If you haven't followed me on Instagram, then do so. Anytime in the future, if there's an update, About the course; if I give any supplementary material or additional problems. So if something happens, I update it on Instagram quickly. So you can check it out. The link is in the description. Now let's check out the notes about ADTs and Arrays. And after that, we'll practise a bit. We'll write some methods; and understand this topic in depth. So guys this is the PDF that I've written for you guys. And with these handwritten PDFs, you'll at least come to know what an abstract data type is What an array is; so here I've defined it well, All you need to remember is In an abstract data type, abstraction means hiding the implementation details. And you are being given an assurance; a provision To carry out these operations. Now there is some minimal functionality, Like Get, Set; I want all of these things in this. But here along with some minimal functionality, There are a set of operations that will be there in this abstract data type. Now I have defined array as an ADT. Its implementation can be done in various programming languages. Array ADT holds a collection of elements It can float, it can be int, it can be classes in C++, and it can be structure in C language. This means that there should be similar elements. The items should be similar. It can't be that one is int and the other is float. I've told this to you in the array. If you guys don't know the array in C language, Then I'll show you which video to watch. I've made a 15-hour video of the C language, You have to watch it. Everything will be clear in that. Learn C with 15 hours and notes; I've told everything in here. I'll just say one thing; the reason why it's 15 hours long Because in it I've made you do a lot of practice programmes and projects. That took a lot of time. But if you look at the concepts, I've done the time stamping, guys Down below in the description, If I open this video, you'll find the time stamp So according to that, you can open the chapters and you can skip the practice set. Then even if you have less time, you'll be set. Okay? So here, we saw all these things on the board; I'd like to tell you one more thing, about static and dynamic arrays. Static arrays are those whose size cannot be changed. But dynamic arrays are those whose size can be changed. Now you'll say, \"We just talked about how the size of an array can't be changed in C language.\" But we can make an ADT, i.e an abstract data type That will implement this functionality. The functionality of dynamic arrays. How will it be in the C language? You can copy the previous elements into the new elements. So we can make an abstract data type of a dynamic array. And that will implement this feature. Okay? So here I have given a quick quiz, To code all the operations for the ADT that I've made Max, Min, Search, Insert and Append. To implement all of these. We'll do this in a while; I've told you about memory representation sufficiently. That the array can be accessed and updated in O(1) time. Because only one calculation has to be done And it will take constant time, regardless of if you're updating the element Or if you want to access it from there. It depends on you and what you want to do. I've written here index 0..1...2...3 It's a contiguous memory location, so from here it will have 10 memory address From here it will have from 10-14 From here it will have from 14-18 From here it will have from 18-22 From here it will have from 22-26. It's 4 bytes each; in every bite, there are 8 bits. In every bit, 0 or 1 is stored. So the computer only speaks in the language of 0 or 1. Here I won't go into too much detail; If you want to know those types of details, I've made a practice playlist of the C language. Here too, videos are going to be added. So you can watch this; if you really want to know all these things, Binary to decimal; decimal to binary, bit operations and Different types of practice programmes for the C language. I've made a separate playlist for that. So you can watch this; I'll be adding a lot of videos here. But if you want to stick to data structures All I'll say is keep following this course And you will definitely like it and it will benefit you a lot. So that is it for this video guys. I hope you have accessed this playlist of data structures. If you haven't then please do and bookmark this playlist. And I want to request you that please; I'm working very hard for this course specifically So if it's possible, please share the videos. I don't think it takes a lot of time to share Simply take the link and forward it to your WhatsApp groups of school, college, or wherever you work. If you do this then it will help me a lot, and I'll bring more of these courses for free with notes. And there are many more things that I'm planning, And if you want to make things easier for me, Please share it as much as you can. Thank you so much guys for watching this video, And I will see you next time."
    },
    {
        "video_id": "jUy5N-3RAjo",
        "title": "Big O Notation, Time Complexity | DSA",
        "url": "https://www.youtube.com/watch?v=jUy5N-3RAjo",
        "duration": 1277,
        "transcript": "in this video we'll talk about time complexity see the thing is in the previous video we have talked about algorithms right now the thing is if you want to build an application basically we are trying to solve a problem right and every problem will have multiple Solutions so the steps which we write to solve that problem that's what we call algorithm example let's say if you want to cook something you follow some steps right now those steps are your algorithm example if I want to record a video we do multiple things we prepare the content we set up this camera mic and then we start recording then we edit the video so there are multiple things involved in the same way if you want to solve any problem we have to follow some steps which we call algorithms and the thing is for one particular problem we don't have one solution we have multiple Solutions and we have to pick the best one now question arise how will you choose the best one so when you say best one what exactly it means so let's say we're on this application I want my application to use less memory or maybe I want when I run this application I want it should take less time or maybe both and that's why we have this term called algorithm analysis so basically you have to analyze your algorithm so that you can make it more efficient right of course as a developer first focus is first the product should work and then you think about optimizing it but when you say optimizing you have to you can optimize in two ways one think about the space complexity and second is the time complexity space complexity simply means can we use an algorithm which will take less memory and time complexity means it will it should take less time but there is one more problem here and the problem is how will you calculate the time or how do you specify the exact time because every machine will have a different output right so if you are testing the algorithm in your machine it might give you different time and then the moment you test this in your office machine it will give you a different time so that's not the best thing to calculate the number of seconds it takes but what you can do is there is a way you can calculate with the help of steps but before we go there let's focus on two algorithms here and then this will make much more sense so the algorithm which we're talking about now is the searching an element in a sorted array so in the previous video we have talked about array and then we also talked about this sorted array so in sorted array by default all the elements will be sorted right now what I would do is I want to search a particular element from this array now there are multiple ways of doing it let's talk about the two so basically we have an option of linear search binary search or many more but let's focus on this too so what exactly linear search and binary searches let's talk about that so what is linear search so let's say you have an array with you okay so this is your array and in this you have multiple values so let's say I'm going with view values here let's have I values and then I'm going to write let's say 5 7 9 12 17. so we got this five values here right and then I want to search a particular element now this is sorted array because you can see all the elements are sorted we got 5 7 9 12 17 and I want to search a particular element let's say I want to search 12. Okay I want to search this element so maybe there is a target value which I want to search in this case the target value is 12. so I want to search 12 right and then this is an array so what we do in the linear search is you go one by one so what you do is in fact we have talked about this in the previous video as well so we'll make it bit faster so what you do is whatever your target value is you will compare that with the first element if it is matching great otherwise you will move to the next element otherwise it will move to the next element otherwise you will move to the next element and I mean of course at this point we don't have to go to the next element so here we basically got the value and once you got the value you can return the value right now there's a problem here the problem is what if the element is not there in the array so of course you have to reach till the end okay what could be the best scenario here so let's say you are searching for an element let's say element is 5 and now we know element 5 with the start itself so when you have the element at the start it will take only one step but what if you have element at the end so it will take the steps depend upon the length of the array so let's say if you have five values it will take five steps when you have 10 values it will take 10 steps when you have one million values okay that's huge let's talk about thousands so let's say when you have 1000 values it will take 1 000 steps that's tricky right now the problem is not it is taking 1000 steps the problem is as your size increases it is also increasing number of steps to search okay that's one parameter you have to remember so that's the theory right now let's understand that with the help of a pseudo code here so you can see I'm writing a pseudo code why pseudo code why not some uh programming language syntax here is because we wanted to make it generic so that you can doesn't matter in which language you work maybe C C plus plus Java JavaScript python maybe COBOL your choice this could pseudo call vitamins I mean saying right now this can be function procedure doesn't matter so what I'm doing is doing doing here is we have a procedure right it can be a function and the name of the function here is the linear search okay now we are accepting a list of items now this is your array the name of the array is a okay and then you are also passing a Target what you're searching so in this case we were searching for 12 but doesn't matter so you have an array with multiple values it can be one value 10 values 1000 values doesn't matter and then you're also passing a Target what you want to search now you will go inside this and then for the first thing you will do is you will want to find the length of the array because of course you should know right till when you have to search so you got a length which is stored in the n and then we are going to basically going from 0 to n minus 1 because we when you start with 0 you have to end with one less right so if you have 10 values you will go till 0 to 9. so you will go from the first value to the last value and then you will search if the target which you are searching is equal to the current element where you are searching so when you are searching for the first element that's the first element here Target is matching if yes you're good you can simply return the value but what if you're not match it's not matching it will go for the next situation next value is it matching yes good return what if it's not matching next value so basically you have to search each value till you're not finding it so that's basically a pseudo code here and what if there is no element example you have an you have five values and you're searching for a value which is not there you can return minus one by specifying hey the value is not there this is simple linear search the code is small but then the time complexity which is the number amount of time it takes is huge okay now let's go for the second approach which is your binary search now what happens in money searches it's very simple basically it's not that simple but let's say so what you do is you have a array again a sorted array and let's say we have multiple values here I'm not sure how many lines I'm going to draw so let's count it later so let's say we got five here again six eight 9 11 13 17. okay so maybe I love odd numbers so it doesn't matter what values you have there so we got uh one two three four five six seven so we got seven values right now what you do is in binary search now since this is sauteed if you're searching for a particular element let's say I'm searching for eight here okay and we know eight is here we know it but computer has no idea where it is so how it is going to search now in linear search what we were doing is we were jumping basically uh from value to Value searching checking this checking this and then goes on so now why why just call this one is because you divide your array into two parts that's the binary is now how you do it so basically what you do is you give a starting point and then you give a ending point and of course every element here will have an index value so in total here we got uh seven values right which goes from zero to six now what you do is you first divide your array into two parts and to do that we have to find a mid value okay so even before you divide you find a mid value so how do you find the mid value so mid value is equal to the starting position plus ending position divided by 2 okay now this is 6 divided by 2 which is three okay so yes your mid now is three and this is your mid okay now basically you check with the mid okay so you always check with the mid is the value which you are searching for which is here is it matching with the mid value which is nine no it's not matching but is it less than or greater than if the value which you're searching for is less than the value which you got as a mid value then the values after the mid value is of no importance right so this values here has no importance you can directly skip them so you you can divide your ad into two parts so you got the first section here which is five six eight nine and then you've got the second section here which is 11 13 17 so what you can do is you can basically skip this part or maybe I will just do that with a color now we can skip this part we don't need this value the value which you want to focus on is 5 6 8 9. so what you do now is you make your mid as the end okay because we have shifted to the left part of the array now after when of course when you shift to the right part of part part of the other in case then you shift your starting value now here you got S and E again and then what you do is you again check your ending you you will find your new new MID value so again S Plus a divided by 2 now s is uh 0 0 plus 3 is 3 by 2 which is one uh of course you can say 1.5 but let's say this is one equal to one we can apply a float function to get the nearest integer okay so we got one here right so what you do is you check so this is your mid again you check is it matching with the value no it's not matching so 8 is not matching with six then what you do then you again divide your array into two parts but then which one you have to skip now now we know that the mid is six so we have to jump to the next value I mean we have to look at the right side value so 6 is less than 8 right so I have to look at the right side so now what we're going to do is let's we have divided into two parts right so now we can skip this part and focus on the right side now so your starting position has been moved here now now this is your s and you got e so again you will find a new MID which is S Plus e divided by 2 which is a 1 plus 3 is 4 4 divided by 2 is equal to 2. so your mid now is eight now is it matching yes okay and that's how you got the value now if you might be thinking we have done so many steps right not exactly basically at the start itself we have remove half of the array and which is very important right so let's say if you have other size of 1000 at the start of the first operation you have removed 500 values for the next operation when you do it you remove 250 values so the number of operations which are doing with one eye search is less right and that's how you know that this is better than the linear search of course in linear search if the value which you're searching for is the first value it's very fast right it will take a lot of time for binary search but in general binary search works faster but how do you do that so with this Theory let's talk about the Practical implementation here so let's look look at the code here very fast because we have already done the theory here so what you do is you basically write a function let's say and then you pass a list of values right and then you also pass a Target same thing we have done in the linear search and here then you go with the left and right so basically we have mentioned the start and the end here in this we are taking left and right let's start with zero as we mentioned the starting to start with zero ending ends at the last value okay we have done that and then you run a loop till you find the value of course then what you do is you check the mid value so this is the mid value we have done that mid is the the starting point ending point divided by 2 and then you check is the middle matching with the target if yes you're done you can simply return the value otherwise we have to do something more then you check if the value which you got which is mid value and the target if the mid value is less than the Target in this case you will focus on the right hand side okay so basically you have to shift to the item side so your left your starting point will shift and we have discussed that here right your starting point will shift and then what if the mid value is greater than the target value in that case it will shift to the left position okay so that's what we have doing here and then every time you do that you run the loop again you find a new MID value and that's what we have done multiple times right we found the mid value three times in the same way you're doing you're going to do here and then at the end you will get a value if it is not get if it is not there you will simply return -1 as we have done before so basically by doing this what we are doing is we we have seen two algorithms right now we have to find the time complexity now what exactly time complexity is it's not about number of seconds it takes to run your code it's all about this so time compensation simply means the measure of how the running time of an algorithm increases with the size of input data so basically let's say when you are testing a particular algorithm you're testing for let's say 10 values or 20 values but what if the values goes up what if you have thousand values how well it will affect your time will it increase the time in linear section let's say if it takes five seconds for 10 values will it take 10 seconds for 20 values or will it take more will it take less okay or let's say you have 1 million records will it take 1 million seconds that's the question we have to answer okay now how will you do that and that's where we have something called Big O notation now what happens in Big O notation is we use this particular concept to understand your time complexity of your algorithm if someone says hey your algorithm is not fast enough you have to first find the Big O notation of it so why Big O is because it is represented with the help of O the Big O in bracket you mentioned the order of n or something so basically it is also called order of all Big O notation and if anything will do so how do you represent that represent that with something like this so we have o off and then the value which is 1 will keep changing so there are multiple things here you can say we have O big O of log n we got Big O of n we got Big O of n log n we say Big O of n Square which is quarterly time we got 2 raised to n we got n factorial okay so we got all this notation but how do you find this okay now this is tricky so of course in this particular video it's not you will not understand everything but let's start with the basic now let's talk about the linear search now if you go back to linear search what we were doing there now in linear search when you have five values you take five steps when you have ten values you take 10 steps you when you have 100 values you take 100 steps right but let's say that is for searching right what about reading if you want to read a particular element from the linear search or from the array it's very easy right you specify the index value let's say the index value is 0 1 2 3 4 and then you specify hey I want the value at index 2 so let's say you send nums which is the name of the array and when you say two basically what you get is nine it's very fast so it doesn't matter what is the size of your array it can be 1 million records the time taken to get a particular element with the help of index value is constant right because computer knows where that memory is so in that case I can say the Big O the time complicity for this is one because it is constant okay now see it's not about one step if you're thinking just because I have mentioned that it is only one step it can be three steps as well so let's say if you are reading a particular element and maybe behind the scene it takes three steps it doesn't matter it's constant right it can be one step three step ten steps it should be constant if it is constant you can say o of 1 but what if you are searching element so if you have an array size of let's say 10 it takes 10 steps now you'll be saying hey you know what if your element is at the start itself of course it will take one step but we have to go for the worst case scenario here the worst case is what if the element is at the end what if the element is not even there it will take 10 steps and that's why we can say the order the Big O notation here will be n what is n here the number of elements in the array as it increases the time also increases the time complexity I'm not talking about the actual seconds okay so that's your linear search now coming back to the binary search what could be the Big O notation for banay search see the thing is as the number increases it will not directly increase the number of steps reason for that is let's say we have we have seven values how many steps it takes let's say three steps because we have to find the mid value three times right whatever size increases to double so let's say from 7 we got 14 so what do you think how many steps will increase what if I say only one the reason being even if you have let's say 14 values here let's say we have so many values at this start itself it will be half right you will get seven values after one step and then you basically increase one more step to find the made up so that means when your value increases it will also increase the number of steps taken but not exactly n because in the linear search if your number of values increases by 10 it will increase 10 steps here as the number of value increases it will not increase the steps in the same sequence so what we can say is we can say it is something between constant it's not exactly constant but it is between the O of 1 and O of n so it is between these two and what comes between these two is O of log n now what is log n basically so log n basically means we have a base 2 here by default okay so number of steps it takes so one what exactly this n here so let's say we have eight values here so when you say log 2 of 8 it will give the value which is three okay now white is 3 is because uh log of 8 is simply means it will take to multiply itself three times to get eight okay so as the number increases let's say now instead of eight values we got 16 values it will give you 4 because it will take 2 to multiply itself four times to get 16. right so it will give you 4 that means as your number of value increases from 8 to 16 it will only increase one step right and that's why the binary search is log n so your linear search is O of N and your binary search is O of log n so which will take less time the log of n okay and you can see the graph here as well so this is elements the number of number of elements you have in the array and this is the number of operation it takes okay so number of steps so if your algorithm is following the constant time it means it's here it doesn't matter how many values you increase it will be constant time log n is similar to it's almost same as constant not exactly same as constant but it will take less time so you can see log n is mentioned here it's there that the wave is there you can't say it oh it's there uh so what if you go with Big O of n so as your value increases your time also increases right but there can be some worse scenarios as well there can be some other algorithms which you implement which might take n log of n which is worse than o of n there might be some algorithm which will take o of n Square which is worse than the o n log n so basically if you want to build an algorithm try to make sure that you are here if you are here you are good okay if you are here it's okay if you are here try to make it better if you are here that's the worst scenario that means you're building an application which is not scalable the moment you have five users on your server your server says I'm good the moment you have 1 000 users it will struggle okay so make sure that you when you whichever algorithm you write you follow this of course in the upcoming videos we'll work on some more algorithms we'll see which one matches there but you got the idea right so the time complexity simply means this definition the measure of how the runtime of the algorithm increases with the size of the input data and to represent that we use Big O notation so I hope you got some idea in the upcoming videos will focus more on different algorithms and also find the big annotation for that apart from this we also have some other notations as well we discuss those things in the upcoming videos"
    },
    {
        "video_id": "aWKEBEg55ps",
        "title": "Time &amp; Space Complexity - Big O Notation - DSA Course in Python Lecture 1",
        "url": "https://www.youtube.com/watch?v=aWKEBEg55ps",
        "duration": 1061,
        "transcript": "hey guys Greg here let's talk about time and space complexity as well as Big O notation now when we talk about these things we generally have some sort of input so we'll say we have an array of just the first five numbers now this is a particular example but in general you could have n many values in your array so you could be given one value you could be given two three maybe even a billion so let's say we were given an array of numbers which I will just call capital A and our task was to return a new array which had the square of all the numbers so we'd want to return we'll call it B which is going to be 1 s which is 1 2 squ which is 4 3 squ which is 9 this would be 16 and 25 now in general this would be written as some sort of a function in Python you define a function with DF and you'd give it a name say like Square so we'll just call it the square function and it's going to take an array so we'll just say a RR now I'm not actually going to write this function but you could picture probably what it does the function would make a new array and it's going to create that by looping over all of the elements in a and squaring those we have to visit all of the numbers in a so again if a has n many numbers then our squaring function here it has to Loop over all of these N Things So in general if your array has n many elements then our squaring function will see that it actually has a Time complexity of Big O of n so basically saying that we have to Loop over all of the N Things now let's say we had a different problem with the same same array a here instead of getting the squares of all the numbers what if we wanted all of the pairs of different numbers so that would be one with two one with three one with four and one with five those are all the combinations with one and you don't want the duplicates like 2 one because those are actually the same pair of numbers here you could then start with two so it's going to be two is with three two could be with four two could be with five so those are all the ones with two all the ones starting with three are are just going to be three with four as well as three with five and you could also have four with five is going to be your last pair there these are all the different pairs of numbers in the array now in this example we have five numbers so n is equal to 5 we have way more than five things in the output here so you could think that this is probably going to be a time complexity that's actually much more than just o of n it's not going to be o of n because we're not just doing five things we're doing way more than that and we'll see why let's think about this as two colors we could green as this one and blue is going to be set over here now basically what you would do is have all of them starting with one so you'd have green as here but then blue moves over so we have 1 and two 1 and three 1 and four one and five notice we basically just went through the entire array already even with just this green thing fixed here then this green would move over one and our blue would reset to over here so we'd basically have to go through the array again we'd have to go through most of the array again we'd have 2 3 2 4 and 25 then then green moves only over one position and we go through a lot of the array again we'd have 3 4 35 and then ultimately we'd have four five now this math won't be exact but roughly if we have n positions for the green one because we basically have to start with everything and then for each of those N Things we're basically sliding the blue one across most of the array we're essentially getting an N times an N which is actually going to be written as an N squ so that's why the time complexity of this problem is actually going to be written as Big O of n^ 2 basically n * n so that's some of the intuition but let's talk exactly about what Big O notation actually is well basically what you'd have to do is for some problem that you have you would draw n on the x- axis so that's say like the size of the array that you're given it could have one element it could have two it could have all the way up until like a trillion elements we'll just write this as a million but it could be infinite really and on the y- AIS you would write the time that it takes and don't think about this as Mill seconds in fact most people would actually say this is more so just the number of operations that you have to do the number of different things that your computer is actually doing your program could do just one operation it could do two three again all the way up until we'll just leave it as a million different things but again it's basically infinite now let's give this a little Legend in green we're going to draw something for the square problem where we are just trying to square all of the numbers in the array we saw that that had a Time complexity of Big O of n now let's talk about what what that means well basically if your array has one thing to square then you just need to do one operation you need to square it if your array has two things well you need to square both of those so we do two things if you had three then you would do three if you had four then you would do four and so on basically you can see here it is creating a line every single time we have one more element we have one more thing to do you would draw this as a straight line and in statistics you'd say this is directly proportional the more elements you have to square well the more operations you'd have to perform now this is opposed to our second example which I'll draw in blue here this was where we looked at basically all pairs of numbers and we said that that had a Time complexity of Big O of n^ 2 or n * n now this is what we call a quadratic function or a parabola in math and it's very very steep it basically looks something like this I'm not going to draw the math precisely CU it's a little complicated but it would curve up very sharply like this and very very quickly it starts to get extremely steep so getting all of the pairs of numbers that is way drastically more timec consuming than it is to just get the square of all of the numbers okay so we would say that this algorithm here is Big O of n^2 and we'd say that this one is Big O of N and in other terms we would say that this is a linear function because it draws a line and the blue one is what we'd call a quadratic function it's way slower and way more timec consuming now there's so many different types of functions you could have we could have another one here which is like all triples of numbers so instead of just a pair it would be all basically three pieces which would be a b and c for each of the numbers you could have that and that would have a big O of n cubed that's basically n * n * n it would look maybe something like this where it's like even sharper than that okay but let's get rid of most of our functions here let's get rid of the red and the blue one and let's just look at this green one here let's now learn what this big O actually means here these are just functions now we need to say exactly what the Big O part of this means now what the big o means is essentially can I draw a line that's worse AKA more operations than yours so can I draw a line that's always going to be worse than yours well yes I actually can I just did that and because I could do that that means that it's true that your function is Big O of n now what also counts as a worst line is something like this which is a straight line but then after a point it's always going to be worse so after this point here my blue line here that one's always going to be high higher than yours so even though it's less high in this region over here after some point AKA after some n value here after a particular threshold of the size of your array my line I drew here is always going to be worse than yours and because I could do that that's what it means for your function to be Big O of n now why is this actually useful well if we bring back our Parabola function which I'll draw in red this time this is again going to be all different pairs of numbers and as we said this is going to be a big O of n s time complexity so if we were to draw that maybe it looked something like this so here's a quadratic function well there is no line in the world that you could draw that's going to what we call bound this thing AKA that's higher than it if you were to draw literally any line that ever existed maybe like this one well after a point your red line is going to be higher than it and even if we drew a super steep line like this it's increasing at such a faster rate than my line that after a point your red line is always going to be worse so my blue line can bound our green line I can draw a line worse than that one but I can't draw a line that's worse than a parabola that's just impossible so that means that this all pairs function is definitely not Big O of n but just like we could draw a line that's going to be steeper than the green one after a point we can draw a parabola that's going to be steeper than our other one after a point here so in this region here our red one is higher but after some point here after some value of n our blue Parabola is always going to be higher it's always going to take more operations and therefore that is why we can say that our red function is Big O of n^2 we could draw another Parabola that bounded it now there we were generally talking about time complexity with Big O notation but you can also talk about space complexity so if you were again writing a function that had an array we'll call it a if you wanted to return an array that had the squares of all the numbers well then basically you would have to create a new array called B which is going to be the squares so 1 4 9 1625 just like we said before and this thing takes up space we are actually storing this in the computer so if your array had in general and many things well then we're basically taking up and much space if you have five things and we need to square them and store those five things well that's going to take up you know five much space it's going to take in general n space so you'd say that this algorithm has a big O of n space complexity because we're taking up n much space however what's really interesting is that you actually don't have to create a new array that's just what makes sense to us you can actually just manipulate the original array that we were given so we were given this array that has N Things and you wanted an array that had the squared numbers well you can actually just modify the original input so you could do this as one this is going to be replaced by 4 to be 9 16 and 25 so notice it's the same exact block of memory we were given we didn't use up any new space so this is actually what we call a constant space solution basically meaning that we're not using any extra space or we're only using constant space we'll talk about that in a moment you'd actually say that this takes up o of one space so big O of one that is constant space now there's also certain algorithms that might have a constant time complexity this one does not because you have to Loop over the nend things so it would have a time of Big O of n because we have to Loop over the N Things but the space complexity is constant because you're not using any extra space now let's see an example of what could have a constant time solution so again we'll just use the exact same example here 1 2 3 4 5 but instead of trying to square all of the numbers what if we just wanted a function that returned the first number in the array generally arrays are zero indexed so these are going to be the array positions and if you haven't done much with arrays don't worry we'll get to them but a cool feature of arrays is that you can write something like a at zero that's generally how you'd write it in a program language and that would tell you what the first element of the array was if you did this it would tell you that the number is one and so if your problem was to know what is the first number in the array well that would actually have a Time complexity of Big O of one a constant time complexity because regardless of how big the array is if you had five things 10 things a million things in general if you had N Things in the array that's not part of your big O expression here n is not anywhere in here we're just writing Big O of one to say this has nothing to do with n it is constant with respect to n now a common confusion that people might have is thinking that o of one means you do one thing it doesn't necessarily mean you do one thing if you change this problem to say what are the first three numbers in the array so you'd want to have 1 2 and three well this is still going to be a constant time solution and you can still write that as bigle of one we'll see why in a few minutes but basically what you would do is You' get a at z a at 1 and a at 2 so these these are going to be the first three numbers in the array AKA these three right here if you wanted to know that we could just do that it's going to take three operations to do that and so that still has nothing to do with the size of n whether n had a 100 a million or a billion things you're just doing three things AKA a constant number of things so this is still going to be constant time and you can still write that as bigle of one time and to bring back our previous graph here if you had n on the xaxis and the number of operations which I'll just write time but it's not really in milliseconds it's like the number of things that you do well a big O of n solution would look something like this so this is a line a big O of n^2 solution might look something like this so this is going to be a parabola which is Big O of n^2 a constant time solution would be written as a horizontal line basically meaning as n is increasing this means that n gets bigger the time that it takes is not increasing at all so it's fixed at this point right here regardless of how big the array is it's not going to take any more time and we'll see lots of examples of O of one things for example hashing is going to come up hashing is generally an O of one thing as we saw array indexing is going to be o of one so you can very quickly get the element that's at a particular part of an array and in general just lots of things are o of one if you wanted to print a number so if you wanted to print something like five well you'd say that's o of one because you just have a number and you print it if you had to print n many numbers so if you had like n numbers to print and you had to print all of those well that's going to be Big O of n because you have to Loop through all of them and then print it that many times now there's some important mathematical laws to understand about Big O notation which is basically you can usually write them in a very simple way if you discovered that your function was something like o of n^2 plus maybe 2 n + 1 well why would that come up well maybe your function did all pairs of numbers so that's going to be an N squ thing to do 2 N would come up if you did two separate Loops so if you maybe printed all of the numbers so if you printed all of them well you'd have N Things to print and then if you also say squared all of the numbers so if you had to square each of the numbers well then that's going to be an end thing to do as well so you have one Loop to print all of them that's o of N and then you have another loop to square all of them that's also o of n so basically that would be a 2 N or n plus n thing to do and then maybe you just did like one more thing like you just printed some random number then you might get something like N2 + 2 n + 1 but the main reason why Big O is used so popularly is because this actually just reduces to Big O of N squared and even if this was like a 5 n squ like you got all the pairs of numbers five times for some reason that's still going to just be written as big old of n^2 now there's basically two main reasons for that which is say you had 5 n^2 here that reduces to that that would basically mean you had a really sharp Parabola like this and so it's going to be five times as worse as you know a normal n squ but the laws of Big O say can you draw a worse one well yeah you could just draw something like 6 n^2 so that's just going to be like ramps up a little bit faster and so it's still going to be written as n^2 and for the same reason if your function is like a complicated Parabola this is just a complicated one where you had 5 n^ 2 + 2 n + 1 there's some other weaker components as well easily the worst thing that you're doing here is the n s part or the 5 n squ part this stuff like basically doesn't even matter you have a line piece over here and then you have a really steep Parabola over here so you'd really only care about the strongest component which in this case is going to be the n s if you had something like an N cubed as well or maybe a 3 n cubed as well well then that's going to be the worst piece so that would actually reduce to just Big O of n cubed because you only care about that strongest component and those numbers out in front here that's just saying how strong your curve is but you can always draw one that's going to be stronger than that and for the same reason if you had something that had to do with the alphabet so if you say looked at all of the lowercase English letters well there's a finite or constant number of those things there's particularly 26 of those things and so that would basically mean if you were drawing a line here this would be at 26 operations and as n got bigger in an array well it's still just going to be 26 letters so there's really nothing to do there and so for the same reason you can actually just equate this to be Big O of one they mean the same thing because if this is some constant number well can you draw basically a worse horizontal line well yeah you can just draw one that's basically 27 here and so for that same reason you would just remove those constants out in front instead of 26 it would just basically be one so you could reduce any known constant number to just be one because that's the smallest number that we have now I'm just going to finish things off with this chart here which is just taken from leak code basically this is the order of notations that you generally prefer here so at the very bottom we have o of one which is constant generally that's pretty fast then log n we'll see that when we do binary search and it's very very fast from there it's linear which is O of N and then this one comes up if you're often sorting things so this is going to be o of n log n to sort o of n s is in the horrible region because it is way way worse than n logn and way worse than o of n it's very very Steep and so generally that's going to be like The Brute Force algorithm if you do something that basically just means the slowest weight possible and O of n factorial and 2 to the N these are insanely slow and you only want to do those if that's actually the fastest thing possible okay I hope this was helpful guys drop a like if it was and have a great day bye-bye"
    },
    {
        "video_id": "BgLTDT03QtU",
        "title": "Big-O Notation - For Coding Interviews",
        "url": "https://www.youtube.com/watch?v=BgLTDT03QtU",
        "duration": 1238,
        "transcript": "hey everyone welcome back and let's write some more neat code today so today I want to run through all the common Big O runtime complexities that you'll need for coding interviews and by the way all of the code from this video will be available for free on neetcode.io if you're not familiar with it it's basically a site that I created it's got a ton of free content to help you prepare for coding interviews including code in Python JavaScript Java and C plus and I've also started making courses so far I completed the data structures and algorithms for beginners course and the advanced algorithms course and I just started uploading the system design for beginners course you can use code neat for 10 off lifetime access that means you'll have lifetime access to all current and future courses so first of all what even is Big O time complexity well it's basically a way of analyzing the run time the amount of time it takes for our algorithm to execute as the input size of our algorithm grows typically we can expect as the input what size of our algorithm grows the execution time of the algorithm is also going to grow but it could grow linearly which you know this is a function you might be familiar with we're used to Y equals X in terms of Big O this is represented as Big O of n where n is just a single variable that's our x-axis now this is where we take a different turn from algebra you could have another function like this which is for example n divided by 2 but we actually don't care about the differences when those differences are constant values we only care about the variable here which is n we don't care about the divided by two and that's true for all Big O run times we also don't care if you know we have n plus some constant like five like that might look something like this on our chart it just starts at a different spot and has the same exact slope but we don't care about these constants either so the first runtime we're going to look at is Big O of one I know we said that we don't care about constants but in this case one is a special constant that we do care about and as you can tell from the chart no matter how much our input size grows the time complexity of Big O of one-time algorithms is always the same you can see it's a flat line because the time it takes for these algorithms does not change based on the input size these are the most efficient algorithms so for example if you have an array of values like this it's a constant time operation to add to the end it's also a constant time operation to remove from the end or just to search an arbitrary index just to look up a value at any index whether it's at 0 or 1 or 2 and the same is true typically for a hash map so to insert any value into a hashmap is constant time to look up a key from a hashmap is also constant time and to remove a key from a hashmap is also a constant time operation and the same operations could be applied to Hash sets as well we also have Big O of n which as we discussed is the linear growth scenario so as our input size grows our time is going to grow proportionately so if we're given some array of values and we want to take the sum of all of those values this sum function is going to be a linear time algorithm because we're going to have to go through every single number in the array so with just three values it won't be too bad but as our input size grows so if our array had a million values in it so then the run time of calculating the sum would also grow and of course we know that's going to happen because under the hood we're going to have to Loop through all of the elements so looping through a list of elements is also a linear time algorithm inserting in the middle of an array and removing from the middle of an array is also Big O of N and that's because when we talk about big of and we mean the worst case run time because in some cases like inserting in the middle well for example inserting in the middle of this array could be over here in in which case we would have to you know put a value let's say we're inserting the value 100 we'd put a hundred over here and then move the 3 into the next position and then this would be our new array so in this case we only had to move a single value but what about if we wanted to insert the 100 over here then we would have to insert the 100 here we'd have to move the one to the right we have to move the 2 to the right and we'd have to move the 3 to the right so that this would be our new array and in this example we had to move every single value in the array we know that's an end time operation so since we don't know where we might insert in the middle we take the worst possible case which would be big of and in this case also removing from the middle is going to be similar if we remove from the end it's a constant time operation but if we move from an arbitrary position for example the first position then we would have to shift every other value to the left and we'd end up with an array that looks like this Now searching an array is also a linear time algorithm in the worst case we would have to look at every single element in the array before we realized that the element in this case if we're looking for a hundred doesn't exist it didn't exist anywhere and we had to look through the entire array to figure that out or we might find the element we might even find it in the first position but we take the worst case which is going to be Big O of n another one that I get a lot of questions about if you have a set of values in this case a list of values one two three and you want to heapify them you want to build a heap out of those elements the heapify algorithm actually runs in Big O of end time if you don't have to individually push an element to a heap it runs more efficiently if you can just build it from scratch this is an end time operation a couple more common algorithms the sliding window algorithm also runs in linear time and I'm actually going to quickly go on neetcode.io over to the sliding Windows section and the second problem in the list when you take a look at the code you see that it does have nested Loops but that doesn't necessarily mean the time complexity is N squared a lot of people assume that just because you have nested Loops the time complexity is N squared but that's not always the case the same is true for the monotonic stack algorithm so once again going on neetcode.io to the stack section the daily temperatures problem is a monotonic stack problem and it also has nested Loops but this algorithm actually runs in Big O of end time if you want more details on these problems I recommend going to neetcode.io you can check out the video explanation and I go more in depth to analyze the time complexity in each of these videos next we have Big O of N squared which we kind of just talked about when you have nested Loops the simplest case is if you actually had a two-dimensional array and you wanted to iterate over that array you would need nested Loops to do that so for example if we had a grid like this three by three so you know it's squared so you know this could be any arbit trade Dimension though n by n you want to First go through every Row in this grid and then for every row you want to go through every single position in the grid so the outer loop is going to go through every single row and the inner loop is going to go through every position in each individual row and what's the size of this grid well it's going to be N squared that's how we get the Big O time complexity for that this is pretty simple another case that kind of trips up some beginners is if you just have a single array and you want to iterate through it n time so if you have an array that looks like this of size n you go through it once that means the Big O time complexity is going to be n if you go through it twice the time complexity is going to be 2 times n which we know reduces to just be Big O of n but what if you iterate through this array n times well then the time complexity is going to be n times n which would be N squared but it's actually not very common that you iterate through the entire array and times what's more common is that you iterate through the entire array once and then you iterate through every element in the array except the first position and then you iterate through every element except the first two positions and you keep doing this until you've just iterate through the last element so this is a diagram of doing that you go through four three two and then one so what would be the time complexity of doing this well mathematically this is actually a well-known series and it can be proven that this will roughly equal N squared divided by two but if you're not smart enough or care about that math portion an easier way to identify that would be well here we have a square right we have an N by n Square now what does this portion of the square look like to you to me it looks like half of a square it basically looks like we cut this in half so now you're probably convinced that iterating through an array like this is going to lead us to N squared divided by 2 time complexity and like I said we don't care about constants so this 2 does not matter therefore we say that iterating through an array like this is also N squared a lot of people kind of brush over this and I'm pretty sure most people never actually learn this or care about it but that's okay because you don't really need to anyway but now you know something that probably most people don't we also talked about how inserting into the middle of an array is an end time operation so insertion sort which basically inserts into the middle of an array n times is going to have N squared time complexity as well another one that's pretty similar to N squared so I won't actually draw this one out on the grid but n times M this could mean a two dimensional Matrix that's not necessarily a square so in our example on the left over here we could have some rectangular array that looks like this where we have two rows and we have three columns so in this case n might be the number of rows where M might be the number of columns we actually have two variables this time which is perfectly valid in Big O notation now if we can have N squared y stop there we can also go up to n cubed and we could actually go higher and higher to n to the power of 4 to the power of 5 etc etc but I won't do that it's pretty uncommon to even have n cubed actually for most problems but this is usually the highest that we get to usually you don't have algorithms that go up to n to the power of four but it's possible and I think you would be able to identify that case if you're able to identify these as well of course we could have some kind of three-dimensional data structure more commonly though you'll just have a single array and you'll want to get every unique triplet from that array for example so in this case we would need three nested Loops to iterate through every single triplet from this input array now let's move on to some of the more complicated ones like log n this is where we get more math involved and I bet you most people don't know how to analyze log n time complexity they kind of just memorize is that because usually log n is applied to binary search for example on an array or you could run binary search on a binary search tree these are the two most common algorithms but there's also pushing and popping from a heap which is another common one on an array the reason binary search runs in log n time is because on every iteration of the loop we eliminate half of the elements from like consideration we don't need to search this half of elements for example so then we'd be left with two elements and we could keep cutting this array basically in half until we have nothing remaining that's when we would have completed our binary search so the question is given an array of size n how many times can you cut the value n by two how many times can you divide it by two until it equals one well another way of talking about this would be how many times can you take the value 1 and multiply it by 2 until it's equal to n AKA 2 to the power of what x in this case is equal to n how do we solve this mathematically well by definition that's what log actually is we take the log of both sides and then we realize that X is equal to log n so therefore the number of times you can take an array like this and divide it by 2 is equal to log n in this case the base of our logarithm is going to be 2. so that's where this comes from and the idea is the same when we're talking about binary search trees at least if your BST is balanced because as we search it we're either going to go to the left or to the right looking for some Target value so if we end up going to the right that means we eliminated the entire left half of the tree from consideration and we do this recursively so we start with a tree of size n and we basically remove half of the elements from consideration until we find the Target that we're looking for that's why this is also a log n time algorithm so log n grows pretty slowly you can't tell from this chart but actually for really really large input sizes log n is practically a flat line like the difference between log n and Big O of n is massive I think log base 2 of 4 billion would be something like 32. so even with an input of billions we get a very small two-digit number now we also have n multiplied by log n and this is actually only marginally less efficient than Big O of n it's a lot more efficient than N squared just like how log n is much more efficient than Big O of n the most common algorithms for n log n are sorting for example merge sort and we usually assume that most built-in sorting functions are n log n so if you need to run the built-in function in a coding interview you can basically assume that it's an N log n algorithm heapsort has the same runtime and this kind of illustrates how we can get a runtime like this we know that popping from a heap is a log end time operation and if we have a heap you know of size n and we have to pop from that Heap while that Heap is non-empty then we end up doing n operations which take log n h therefore we get n log n as the total result now in this example before we started running the heaps or algorithm we actually had to build the Heap so this took Big O of n times so actually the overall run time was n plus n log n should we draw that on our chart well with Big O notation we only care about the larger term if we have a term like this which is strictly larger than this term we don't care about the smaller term but we could have a Time complexity where we have M plus n log n and in this case since m is a different variable from any of the variables we have here we don't know that this is a strictly smaller than this one so we have to include both of these in the Big O time complexity but we don't have that in this case so we reduce the time complexity to just be n log n now let's get into some of the non-polynomial runtimes which starting with the most common is 2 to the power of n and this is the part where I could start going on a nerd tangent and talking about how this equation is actually just a reflection of the log n over you know this axis but I'm pretty sure nobody cares about that so I won't get into it but feel free to comment if you're interested in the math stuff it's most common to have a runtime like this when you're talking about recursion so this is an example of two Branch recursion where our recursive tree will have a height of n but we'll have two branches every time so this is recursively calling some recursive function it's not really doing anything but here you can see we have two branches of recursion visually it would look something like this you know similar to a binary tree but in this case the height of this tree would be measured in terms of n so for example if we were given some input array of size n and we wanted to get all possible combinations that would be one example of 2 to the power of n run time so for example Computing the Fibonacci sequence recursively is a pretty common example or combinations now if we can have 2 to the power of n can't we have 3 to the power of n and what about four to the power of n yeah you can have pretty much any constant let's call it C raised to a power of n now I'm not going to draw those out but with a bigger base value we will have you know increased runtime complexity so an example of that would be pretty similar to our previous recursive example but in this case instead of having two branches we have an arbitrary number of branches so we use a loop to do that so in this case our constant C is going to tell us how many times we need to Loop and make a recursive path so in this case if C was three we'd have a decision tree with three branches and the height of the tree would continue to be n so we would end up with a time complexity of 3 to the power of n now how am I getting that time complexity from a decision tree well think about it this way if we have three uh you know branches on the first level we have three values here now for each of those we're going to have three more branches and then for each of those we're going to have three more branches we're basically going to keep doing this until we reach the entire height of the tree which in this case is n so we're basically going to have a number our base value 3 multiplied by three and just keep multiplying it until we have n of these therefore we get 3 to the power of n now a pretty rare runtime complexity is the square root of n you rarely see this but I still think it's worth mentioning you will rarely ever see this in a coding interview but I think it's still worth mentioning the only example I think I've ever needed this for is getting all the factors of some value so for example if we're given a value n and we want to get all the factors of it we will go through every value between 1 and the square root of N and check if any of those values is a factor of N and since this could possibly produce some duplicate factors we add those factors to a hash set which we know is a constant time operation so the number of times this Loop is going to execute is of course going to be the square root of n now I won't go super in depth on why this algorithm works if you want to know feel free to ask in the comments it's really more of a math problem than a coding problems and lastly we have n factorial which is a math equation which basically means you know if we had 5 factorial that is 5 times 4 times 3 times 2 times 1. so basically it ends up being a very big number we say that this is bigger than 2 to the power of n because 2 to the power of 5 in this case would be two times two times two a five times and you can see that just by looking at these two since factorial has the potential for having larger terms than just some constant value two or three or four that this is going to grow much quicker than this one now n factorial is pretty rare it mainly comes up for permutations and sometimes in graph problems for example the traveling salesman problem but I think you don't have to focus too much on N factorial you mainly have to know that it's very inefficient so if your solution is n factorial most likely you don't have the optimal solution but sometimes n factorial is the best you can do in a real interview I don't think it would matter too much if you can you know precisely tell is your algorithm n factorial or is it exponential to the power of n it wouldn't matter a lot and also knowing the exact difference for example the square root of n is actually growing quicker than log n you could kind of figure that out just by doing some math in your head like I said the log of 4 billion is going to be something like 32 whereas the square root of 4 billion is still going to be a very large number but this is Main only to give you a high level overview of how these time complexities relate to each other because that's what's important you need to be able to know how efficient your algorithm is if you see your algorithm is factorial or exponential or even n cubed you know it's not super efficient you want your algorithms to be on the right side of this chart though sometimes it's not possible if you found this video helpful please like And subscribe it really supports the channel a lot if you're preparing for coding interviews check out neatco.io it has a ton of free resources you can also get 10 off lifetime access using Code neat thanks for watching and hopefully I'll see you pretty soon"
    }
]